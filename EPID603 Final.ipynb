{
 "metadata": {
  "name": "",
  "signature": "sha256:5b7ac0d618711d2424bd3db67bb706efa2026cbb4b2e8cddc231ccf72fcf2665"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Set Up#\n",
      "Here we import and load the needed libraries and magic cells for analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import networkx as nx\n",
      "import scipy\n",
      "from scipy.integrate import odeint\n",
      "from scipy import integrate\n",
      "from scipy.optimize import fmin\n",
      "%matplotlib inline\n",
      "%load_ext rmagic\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The rmagic extension is already loaded. To reload it, use:\n",
        "  %reload_ext rmagic\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Handeling#\n",
      "These cell select and format data held on a thumbdrive.  The relabund_plot function can select OTU's based either on a percent cut off or a number, as in selecting the two OTU's that have the highest average relative abundance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "relabund <-read.table('/Volumes/MCCRONE/603 Project/Data/simple.relabund.metadata', header=T,sep='\\t',strip.white=TRUE)\n",
      "relabund <- relabund[,c(2:4,6:ncol(relabund))]\n",
      " source(\"~/Desktop/GF_analysis/relabund_plots.R\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "Loading required package: gtools\n",
        "Loading required package: gdata\n",
        "gdata: read.xls support for 'XLS' (Excel 97-2004) files ENABLED.\n",
        "\n",
        "gdata: read.xls support for 'XLSX' (Excel 2007+) files ENABLED.\n",
        "\n",
        "Attaching package: \u2018gdata\u2019\n",
        "\n",
        "The following object(s) are masked from \u2018package:stats\u2019:\n",
        "\n",
        "    nobs\n",
        "\n",
        "The following object(s) are masked from \u2018package:utils\u2019:\n",
        "\n",
        "    object.size\n",
        "\n",
        "Loading required package: caTools\n",
        "Loading required package: grid\n",
        "Loading required package: KernSmooth\n",
        "KernSmooth 2.23 loaded\n",
        "Copyright M. P. Wand 1997-2009\n",
        "Loading required package: MASS\n",
        "\n",
        "Attaching package: \u2018gplots\u2019\n",
        "\n",
        "The following object(s) are masked from \u2018package:stats\u2019:\n",
        "\n",
        "    lowess\n",
        "\n",
        "\n",
        "Attaching package: \u2018plotrix\u2019\n",
        "\n",
        "The following object(s) are masked from \u2018package:gplots\u2019:\n",
        "\n",
        "    plotCI\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "x<-relabund_plot('A',relabund,'count',5)\n",
      "means<- t(x[[1]])\n",
      "\n",
      "errors<- t(x[[2]]) # Means errors and days are saved so they can be used in the python analyis below\n",
      "\n",
      "days<- x[[3]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAYAAAB91L6VAAAEJGlDQ1BJQ0MgUHJvZmlsZQAAOBGF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\n"
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Model Setup #\n",
      "Here I load the data from R and set up the pre C.diff gut model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get Data\n",
      "means = %Rget means \n",
      "errors = %Rget errors\n",
      "Td = %Rget days"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set up just for the pre C.Diff\n",
      "Td =Td[0:11]+22  # Selects just the days prior to 0 and makes them all positive\n",
      "#Here I save the time course of each OTU as a separate array\n",
      "OTU1d =means[0][0:11]\n",
      "OTU2d =means[1][0:11]\n",
      "OTU3d =means[2][0:11]\n",
      "OTU4d =means[3][0:11]\n",
      "OTU5d =means[4][0:11]\n",
      "#OTU6d =means[5][0:11]\n",
      "#OTU7d =means[6][0:11]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And we'll plot everything as dots just to make sure I'm not crazy and nothing was changed while passing into python\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure()\n",
      "plt.plot(Td,OTU1d,'o',Td,OTU2d,'o',Td,OTU3d,'o',Td,OTU4d,'o',Td,OTU5d,'o')#,Td,OTU6d,'o',Td,OTU7d,'o')\n",
      "plt.xlabel('days')\n",
      "plt.ylabel('relative abundance')\n",
      "title='Pre C. difficile challenge'\n",
      "plt.title(title)\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEZCAYAAACXRVJOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10c7991d0>"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Setting up the equations #\n",
      "The model is saved as a function eq which intakes the parameters, initial conditions, the start time, end time and the number of increments over that time span.  It house the group of equations that make up the heart of the ode and the integrator."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#The Model\n",
      "#=======================================================\n",
      "def eq(par,initial_cond,start_t,end_t,incr):\n",
      "    #-time-grid-----------------------------------\n",
      "    t  = np.linspace(start_t, end_t,incr)\n",
      "    #differential-eq-system---------------------\n",
      "    def funct(y,t):\n",
      "        OTU1i = y[0]\n",
      "        OTU2i = y[1]\n",
      "        OTU3i = y[2]\n",
      "        #OTU4i = y[3]\n",
      "        #OTU5i = y[4]\n",
      "        #OTU6i = y[5]\n",
      "        #OTU7i = y[6] \n",
      "\n",
      "        #Parameters\n",
      "        #a1,a2,a3,a4,a5,a6,a7,b12,b13,b14,b15,b16,b17,b21,b23,b24,b25,b26,b27,b31,b32,b34,b35,b36,b36,b37,b41,b42,b43,b45,b46,b47,b51,b52,b53,b54,b56,b57,b61,b62,b63,b64,b65,b67,b71,b72,b73,b74,b75,b76 = par \n",
      "        a1,a2,a3,b12,b13,b21,b23,b31,b32 = par \n",
      "\n",
      "\n",
      "        OTU1 = a1*OTU1i*(1-(OTU1i+OTU2i+OTU3i)) + OTU1i*(b12*OTU2i+b13*OTU3i)#+b14*OTU4i+b15*OTU5i+b16*OTU6i+b17*OTU7i)\n",
      "        OTU2 = a2*OTU2i*(1-(OTU2i+OTU1i+OTU2i)) + OTU2i*(b21*OTU1i+b23*OTU3i)#+b24*OTU4i+b25*OTU5i+b26*OTU6i+b27*OTU7i)\n",
      "        OTU3 = a3*OTU3i*(1-(OTU3i+OTU2i+OTU1i)) + OTU3i*(b31*OTU1i+b32*OTU2i)#+b34*OTU4i+b35*OTU5i+b36*OTU6i+b37*OTU7i)\n",
      "        #OTU4 = a4*OTU4i*(1-(OTU4i))+ OTU4i*(b41*OTU1i+b42*OTU2i+b43)#*OTU3i+b45*OTU5i+b46*OTU6i+b47*OTU7i)\n",
      "        #OTU5 = a5*OTU5i*(1-(OTU5i)) + OTU5i*(b51*OTU1i+b52*OTU2i+b53*OTU3i+b54*OTU4i+b56*OTU6i+b57*OTU7i)\n",
      "        #OTU6 = a6*OTU6i*(1-(OTU6i)) + OTU6i*(b61*OTU1i+b62*OTU2i+b63*OTU3i+b64*OTU4i+b65*OTU5i+b67*OTU7i)\n",
      "        #OTU7 = a7*OTU7i*(1-(OTU7i)) + OTU7i*(b71*OTU1i+b72*OTU2i+b73*OTU3i+b74*OTU4i+b75*OTU5i+b76*OTU6i)\n",
      "        \n",
      "        \n",
      "        return [OTU1,OTU2,OTU3]#,OTU4,OTU5,OTU6,OTU7]\n",
      "        \n",
      "        \n",
      "   #integrate------------------------------------\n",
      "    ds = integrate.odeint(funct,initial_cond,t)\n",
      "    return (ds[:,0],ds[:,1],ds[:,2],t)#,ds[:,3],ds[:,4],ds[:,5],ds[:,6],t)\n",
      "#=======================================================\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Parameters#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#===================================================\n",
      "# model parameters\n",
      "#----------------------------------------------------\n",
      "a1=1\n",
      "a2=2\n",
      "a3=0.9\n",
      "a4=0\n",
      "a5=0\n",
      "a6=0\n",
      "a7=0\n",
      "\n",
      "b12=-.2\n",
      "b13=-.4\n",
      "b14=0\n",
      "b15=0\n",
      "b16=0\n",
      "b17=0\n",
      "\n",
      "b21=-.1\n",
      "b23=-.4\n",
      "b24=0\n",
      "b25=0\n",
      "b26=0\n",
      "b27=0\n",
      "\n",
      "b31=0\n",
      "b32=0\n",
      "b34=0\n",
      "b35=0\n",
      "b36=0\n",
      "b37=0\n",
      "\n",
      "b41=0\n",
      "b42=0\n",
      "b43=0\n",
      "b45=0\n",
      "b46=0\n",
      "b47=0\n",
      "\n",
      "b51=0\n",
      "b52=0\n",
      "b53=0\n",
      "b54=0\n",
      "b56=0\n",
      "b57=0\n",
      "\n",
      "b61=0\n",
      "b62=0\n",
      "b63=0\n",
      "b64=0\n",
      "b65=0\n",
      "b67=0\n",
      "\n",
      "b71=0\n",
      "b72=0\n",
      "b73=0\n",
      "b74=0\n",
      "b75=0\n",
      "b76=0\n",
      "#rates=(a1,a2,a3,a4,a5,a6,a7,b12,b13,b14,b15,b16,b17,b21,b23,b24,b25,b26,b27,b31,b32,b34,b35,b36,b36,b37,b41,b42,b43,b45,b46,b47,b51,b52,b53,b54,b56,b57,b61,b62,b63,b64,b65,b67,b71,b72,b73,b74,b75,b76)\n",
      "rates = (a1,a2,a3,b12,b13,b21,b23,b31,b32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Initial Conditions #\n",
      "The initial conditions are set as the first term in the data array imported from R.  Technically this is day 0, everything that made it through the gauvaging step. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model initial conditions\n",
      "#---------------------------------------------------\n",
      "OTU1o = OTU1d[0]               \n",
      "OTU2o = OTU2d[0]                  \n",
      "OTU3o = OTU3d[0]\n",
      "#OTU4o = OTU4d[0]\n",
      "#OTU5o = OTU5d[0]\n",
      "#OTU6o = OTU6d[0]\n",
      "#OTU7o = OTU7d[0]\n",
      "y0 = [OTU1o, OTU2o,OTU3o]#,OTU4o,OTU5o,OTU6o,OTU7o]      \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model steps\n",
      "#---------------------------------------------------\n",
      "start_time=0.0\n",
      "end_time=22\n",
      "intervals=1000\n",
      "mt=np.linspace(start_time,end_time,intervals)   #evenly spaced numbers over the time space\n",
      " \n",
      "# model index to compare to data\n",
      "#----------------------------------------------------\n",
      "findindex=lambda x:np.where(mt>=x)[0][0]\n",
      "mindex=map(findindex,Td)\n",
      "#======================================================="
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#3.Score Fit of System\n",
      "#=========================================================\n",
      "def score(parms):\n",
      "    #a.Get Solution to system\n",
      "    #OTU1,OTU2,OTU3,OTU4,OTU5,OTU6,OTU7,T=eq(rates,y0,start_time,end_time,intervals)    #b.Pick of Model Points to Compare\n",
      "    OTU1,OTU2,OTU3,T=eq(parms,y0,start_time,end_time,intervals)\n",
      "    \n",
      "    OTU1m=OTU1[mindex]\n",
      "    OTU2m=OTU2[mindex]\n",
      "    OTU3m=OTU3[mindex]\n",
      "    #OTU4m=OTU4[mindex]\n",
      "    #OTU5m=OTU5[mindex]\n",
      "    #OTU6m=OTU6[mindex]\n",
      "    #OTU7m=OTU7[mindex]\n",
      "        \n",
      "    #c.Score Difference between model and data points\n",
      "    ss=lambda data,model:sum(((data-model)**2))\n",
      "   \n",
      "    #totalss = ss(OTU1d,OTU1m)/np.mean(OTU1m) + ss(OTU2d,OTU2m)/np.mean(OTU2m) + ss(OTU3d,OTU3m)/np.mean(OTU3m) #+ ss(OTU4d,OTU4m)/np.mean(OTU4m) + ss(OTU5d,OTU5m)/np.mean(OTU5m) + ss(OTU6d,OTU6m)/np.mean(OTU6m) + ss(OTU7d,OTU7m)/np.mean(OTU7m)\n",
      "    totalss = ss(OTU1d,OTU1m) + ss(OTU2d,OTU2m) + ss(OTU3d,OTU3m) #+ ss(OTU4d,OTU4m)/np.mean(OTU4m) + ss(OTU5d,OTU5m)/np.mean(OTU5m) + ss(OTU6d,OTU6m)/np.mean(OTU6m) + ss(OTU7d,OTU7m)/np.mean(OTU7m)\n",
      "    \n",
      "    return totalss\n",
      "#========================================================"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#4.Optimize Fit\n",
      "#=======================================================\n",
      "answ=fmin(score,(rates),full_output=1,maxfun=1000000, maxiter = 1000000)\n",
      "bestrates=answ[0]\n",
      "bestscore=answ[1]\n",
      "#a1,a2,a3,a4,a5,a6,a7,b12,b13,b14,b15,b16,b17,b21,b23,b24,b25,b26,b27,b31,b32,b34,b35,b36,b36,b37,b41,b42,b43,b45,b46,b47,b51,b52,b53,b54,b56,b57,b61,b62,b63,b64,b65,b67,b71,b72,b73,b74,b75,b76=answ[0]\n",
      "#newrates=(a1,a2,a3,a4,a5,a6,a7,b12,b13,b14,b15,b16,b17,b21,b23,b24,b25,b26,b27,b31,b32,b34,b35,b36,b36,b37,b41,b42,b43,b45,b46,b47,b51,b52,b53,b54,b56,b57,b61,b62,b63,b64,b65,b67,b71,b72,b73,b74,b75,b76)\n",
      "a1,a2,a3,b12,b13,b21,b23,b31,b32=answ[0]\n",
      "newrates=(a1,a2,a3,b12,b13,b21,b23,b31,b32)\n",
      "print newrates\n",
      "#======================================================="
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.183798\n",
        "         Iterations: 1710\n",
        "         Function evaluations: 2564\n",
        "(7.1399090108295242, 10.00794541349433, 0.050359187435003817, -7.6029081057630421, 10.776731982145737, -2.7977891716086303, 26.749879619427723, -0.34901160174568147, 0.58428337188426283)\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Running the model #\n",
      "This is where the model is finally run to generate the solutions.  At this point we are using the parameters which have been optimize above (hopefully)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#5.Generate Solution to System\n",
      "#=======================================================\n",
      "#OTU1,OTU2,OTU3,OTU4,OTU5,OTU6,OTU7,T=eq(rates,y0,start_time,end_time,intervals)\n",
      "OTU1,OTU2,OTU3,T=eq(newrates,y0,start_time,end_time,intervals)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#6. Plot Solution to System\n",
      "#=========================================================\n",
      "plt.figure()\n",
      "plt.plot(T,OTU1,T,OTU2,T,OTU3,Td,OTU1d,'bo',Td,OTU2d,'go',Td,OTU3d,'ro')#,T,OTU3,T,OTU4,T,OTU5,T,OTU6,T,OTU7)\n",
      "plt.xlabel('days')\n",
      "plt.ylabel('relative abundance')\n",
      "title='Pre C. difficile challenge'\n",
      "\n",
      "plt.title(title)\n",
      "plt.show()\n",
      "#========================================================="
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEZCAYAAACXRVJOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10c09c6d0>"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "newrates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "(-1.1915505462021723,\n",
        " 603.05516168372628,\n",
        " 22.618883363343834,\n",
        " 2.4463690073292668,\n",
        " -4.970436829184937,\n",
        " 1.6182097656390946,\n",
        " -0.82057872356686268,\n",
        " 10.480764795259622,\n",
        " -23.552981098279446)"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DistMatrix =np.array([[0,      0.3,    0.4,    0.7],\n",
      "[0.3,    0,      0.9,    0.2],\n",
      "[0.4,    0.9,    0,      0.1],\n",
      "[0.7,    0.2,    0.1,    0] ])\n",
      "G = G=nx.from_numpy_matrix(DistMatrix)\n",
      "nx.draw(G)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFBCAYAAAA2bKVrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1097dd810>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mindex\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[0, 46, 91, 137, 182, 228, 273, 318, 364, 682, 999]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "OTU1m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'OTU1m' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-58-e1373be7cadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOTU1m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'OTU1m' is not defined"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(scipy.optimize)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on package scipy.optimize in scipy:\n",
        "\n",
        "NAME\n",
        "    scipy.optimize\n",
        "\n",
        "FILE\n",
        "    /Users/mccrone/anaconda/lib/python2.7/site-packages/scipy/optimize/__init__.py\n",
        "\n",
        "DESCRIPTION\n",
        "    =====================================================\n",
        "    Optimization and root finding (:mod:`scipy.optimize`)\n",
        "    =====================================================\n",
        "    \n",
        "    .. currentmodule:: scipy.optimize\n",
        "    \n",
        "    Optimization\n",
        "    ============\n",
        "    \n",
        "    General-purpose\n",
        "    ---------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize - Unified interface for minimizers of multivariate functions\n",
        "       fmin - Nelder-Mead Simplex algorithm\n",
        "       fmin_powell - Powell's (modified) level set method\n",
        "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm\n",
        "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno)\n",
        "       fmin_ncg - Line-search Newton Conjugate Gradient\n",
        "       leastsq - Minimize the sum of squares of M equations in N unknowns\n",
        "    \n",
        "    Constrained (multivariate)\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer\n",
        "       fmin_tnc - Truncated Newton code\n",
        "       fmin_cobyla - Constrained optimization by linear approximation\n",
        "       fmin_slsqp - Minimization using sequential least-squares programming\n",
        "       nnls - Linear least-squares problem with non-negativity constraint\n",
        "    \n",
        "    Global\n",
        "    ------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       anneal - Simulated annealing\n",
        "       basinhopping - Basinhopping stochastic optimizer\n",
        "       brute - Brute force searching optimizer\n",
        "    \n",
        "    Scalar function minimizers\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize_scalar - Unified interface for minimizers of univariate functions\n",
        "       fminbound - Bounded minimization of a scalar function\n",
        "       brent - 1-D function minimization using Brent method\n",
        "       golden - 1-D function minimization using Golden Section method\n",
        "       bracket - Bracket a minimum, given two starting points\n",
        "    \n",
        "    Rosenbrock function\n",
        "    -------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       rosen - The Rosenbrock function.\n",
        "       rosen_der - The derivative of the Rosenbrock function.\n",
        "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
        "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
        "    \n",
        "    Fitting\n",
        "    =======\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       curve_fit -- Fit curve to a set of points\n",
        "    \n",
        "    Root finding\n",
        "    ============\n",
        "    \n",
        "    Scalar functions\n",
        "    ----------------\n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       brentq - quadratic interpolation Brent method\n",
        "       brenth - Brent method, modified by Harris with hyperbolic extrapolation\n",
        "       ridder - Ridder's method\n",
        "       bisect - Bisection method\n",
        "       newton - Secant method or Newton's method\n",
        "    \n",
        "    Fixed point finding:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fixed_point - Single-variable fixed-point solver\n",
        "    \n",
        "    Multidimensional\n",
        "    ----------------\n",
        "    \n",
        "    General nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       root - Unified interface for nonlinear solvers of multivariate functions\n",
        "       fsolve - Non-linear multi-variable equation solver\n",
        "       broyden1 - Broyden's first method\n",
        "       broyden2 - Broyden's second method\n",
        "    \n",
        "    Large-scale nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       newton_krylov\n",
        "       anderson\n",
        "    \n",
        "    Simple iterations:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       excitingmixing\n",
        "       linearmixing\n",
        "       diagbroyden\n",
        "    \n",
        "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
        "    \n",
        "    Utility Functions\n",
        "    =================\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       line_search - Return a step that satisfies the strong Wolfe conditions\n",
        "       check_grad - Check the supplied derivative using finite differences\n",
        "    \n",
        "       show_options - Show specific options optimization solvers\n",
        "\n",
        "PACKAGE CONTENTS\n",
        "    _basinhopping\n",
        "    _cobyla\n",
        "    _lbfgsb\n",
        "    _minimize\n",
        "    _minpack\n",
        "    _nnls\n",
        "    _root\n",
        "    _slsqp\n",
        "    _trustregion\n",
        "    _trustregion_dogleg\n",
        "    _trustregion_ncg\n",
        "    _tstutils\n",
        "    _zeros\n",
        "    anneal\n",
        "    cobyla\n",
        "    lbfgsb\n",
        "    linesearch\n",
        "    minpack\n",
        "    minpack2\n",
        "    moduleTNC\n",
        "    nnls\n",
        "    nonlin\n",
        "    optimize\n",
        "    setup\n",
        "    slsqp\n",
        "    tnc\n",
        "    zeros\n",
        "\n",
        "CLASSES\n",
        "    __builtin__.dict(__builtin__.object)\n",
        "        scipy.optimize.optimize.Result\n",
        "    exceptions.UserWarning(exceptions.Warning)\n",
        "        scipy.optimize.optimize.OptimizeWarning\n",
        "    \n",
        "    class OptimizeWarning(exceptions.UserWarning)\n",
        "     |  Method resolution order:\n",
        "     |      OptimizeWarning\n",
        "     |      exceptions.UserWarning\n",
        "     |      exceptions.Warning\n",
        "     |      exceptions.Exception\n",
        "     |      exceptions.BaseException\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __delattr__(...)\n",
        "     |      x.__delattr__('name') <==> del x.name\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __getslice__(...)\n",
        "     |      x.__getslice__(i, j) <==> x[i:j]\n",
        "     |      \n",
        "     |      Use of negative indices is not supported.\n",
        "     |  \n",
        "     |  __reduce__(...)\n",
        "     |  \n",
        "     |  __repr__(...)\n",
        "     |      x.__repr__() <==> repr(x)\n",
        "     |  \n",
        "     |  __setattr__(...)\n",
        "     |      x.__setattr__('name', value) <==> x.name = value\n",
        "     |  \n",
        "     |  __setstate__(...)\n",
        "     |  \n",
        "     |  __str__(...)\n",
        "     |      x.__str__() <==> str(x)\n",
        "     |  \n",
        "     |  __unicode__(...)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |  \n",
        "     |  args\n",
        "     |  \n",
        "     |  message\n",
        "    \n",
        "    class Result(__builtin__.dict)\n",
        "     |  Represents the optimization result.\n",
        "     |  \n",
        "     |  Attributes\n",
        "     |  ----------\n",
        "     |  x : ndarray\n",
        "     |      The solution of the optimization.\n",
        "     |  success : bool\n",
        "     |      Whether or not the optimizer exited successfully.\n",
        "     |  status : int\n",
        "     |      Termination status of the optimizer. Its value depends on the\n",
        "     |      underlying solver. Refer to `message` for details.\n",
        "     |  message : str\n",
        "     |      Description of the cause of the termination.\n",
        "     |  fun, jac, hess, hess_inv : ndarray\n",
        "     |      Values of objective function, Jacobian, Hessian or its inverse (if\n",
        "     |      available). The Hessians may be approximations, see the documentation\n",
        "     |      of the function in question.\n",
        "     |  nfev, njev, nhev : int\n",
        "     |      Number of evaluations of the objective functions and of its\n",
        "     |      Jacobian and Hessian.\n",
        "     |  nit : int\n",
        "     |      Number of iterations performed by the optimizer.\n",
        "     |  maxcv : float\n",
        "     |      The maximum constraint violation.\n",
        "     |  \n",
        "     |  Notes\n",
        "     |  -----\n",
        "     |  There may be additional attributes not listed above depending of the\n",
        "     |  specific solver. Since this class is essentially a subclass of dict\n",
        "     |  with attribute accessors, one can see which attributes are available\n",
        "     |  using the `keys()` method.\n",
        "     |  \n",
        "     |  Method resolution order:\n",
        "     |      Result\n",
        "     |      __builtin__.dict\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __delattr__ = __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __getattr__(self, name)\n",
        "     |  \n",
        "     |  __repr__(self)\n",
        "     |  \n",
        "     |  __setattr__ = __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |      dictionary for instance variables (if defined)\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __cmp__(...)\n",
        "     |      x.__cmp__(y) <==> cmp(x,y)\n",
        "     |  \n",
        "     |  __contains__(...)\n",
        "     |      D.__contains__(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __eq__(...)\n",
        "     |      x.__eq__(y) <==> x==y\n",
        "     |  \n",
        "     |  __ge__(...)\n",
        "     |      x.__ge__(y) <==> x>=y\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __gt__(...)\n",
        "     |      x.__gt__(y) <==> x>y\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  __iter__(...)\n",
        "     |      x.__iter__() <==> iter(x)\n",
        "     |  \n",
        "     |  __le__(...)\n",
        "     |      x.__le__(y) <==> x<=y\n",
        "     |  \n",
        "     |  __len__(...)\n",
        "     |      x.__len__() <==> len(x)\n",
        "     |  \n",
        "     |  __lt__(...)\n",
        "     |      x.__lt__(y) <==> x<y\n",
        "     |  \n",
        "     |  __ne__(...)\n",
        "     |      x.__ne__(y) <==> x!=y\n",
        "     |  \n",
        "     |  __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  __sizeof__(...)\n",
        "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
        "     |  \n",
        "     |  clear(...)\n",
        "     |      D.clear() -> None.  Remove all items from D.\n",
        "     |  \n",
        "     |  copy(...)\n",
        "     |      D.copy() -> a shallow copy of D\n",
        "     |  \n",
        "     |  fromkeys(...)\n",
        "     |      dict.fromkeys(S[,v]) -> New dict with keys from S and values equal to v.\n",
        "     |      v defaults to None.\n",
        "     |  \n",
        "     |  get(...)\n",
        "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
        "     |  \n",
        "     |  has_key(...)\n",
        "     |      D.has_key(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  items(...)\n",
        "     |      D.items() -> list of D's (key, value) pairs, as 2-tuples\n",
        "     |  \n",
        "     |  iteritems(...)\n",
        "     |      D.iteritems() -> an iterator over the (key, value) items of D\n",
        "     |  \n",
        "     |  iterkeys(...)\n",
        "     |      D.iterkeys() -> an iterator over the keys of D\n",
        "     |  \n",
        "     |  itervalues(...)\n",
        "     |      D.itervalues() -> an iterator over the values of D\n",
        "     |  \n",
        "     |  keys(...)\n",
        "     |      D.keys() -> list of D's keys\n",
        "     |  \n",
        "     |  pop(...)\n",
        "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
        "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
        "     |  \n",
        "     |  popitem(...)\n",
        "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
        "     |      2-tuple; but raise KeyError if D is empty.\n",
        "     |  \n",
        "     |  setdefault(...)\n",
        "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
        "     |  \n",
        "     |  update(...)\n",
        "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
        "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
        "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
        "     |      In either case, this is followed by: for k in F: D[k] = F[k]\n",
        "     |  \n",
        "     |  values(...)\n",
        "     |      D.values() -> list of D's values\n",
        "     |  \n",
        "     |  viewitems(...)\n",
        "     |      D.viewitems() -> a set-like object providing a view on D's items\n",
        "     |  \n",
        "     |  viewkeys(...)\n",
        "     |      D.viewkeys() -> a set-like object providing a view on D's keys\n",
        "     |  \n",
        "     |  viewvalues(...)\n",
        "     |      D.viewvalues() -> an object providing a view on D's values\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __hash__ = None\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "\n",
        "FUNCTIONS\n",
        "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using (extended) Anderson mixing.\n",
        "        \n",
        "        The Jacobian is formed by for a 'best' solution in the space\n",
        "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
        "        inversions and MxN multiplications are required. [Ey]_\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        M : float, optional\n",
        "            Number of previous vectors to retain. Defaults to 5.\n",
        "        w0 : float, optional\n",
        "            Regularization parameter for numerical stability.\n",
        "            Compared to unity, good values of the order of 0.01.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
        "    \n",
        "    anneal(func, x0, args=(), schedule='fast', full_output=0, T0=None, Tf=1e-12, maxeval=None, maxaccept=None, maxiter=400, boltzmann=1.0, learn_rate=0.5, feps=1e-06, quench=1.0, m=1.0, n=1.0, lower=-100, upper=100, dwell=50, disp=True)\n",
        "        Minimize a function using simulated annealing.\n",
        "        \n",
        "        Uses simulated annealing, a random algorithm that uses no derivative\n",
        "        information from the function being optimized. Other names for this\n",
        "        family of approaches include: \"Monte Carlo\", \"Metropolis\",\n",
        "        \"Metropolis-Hastings\", `etc`. They all involve (a) evaluating the\n",
        "        objective function on a random set of points, (b) keeping those that\n",
        "        pass their randomized evaluation critera, (c) cooling (`i.e.`,\n",
        "        tightening) the evaluation critera, and (d) repeating until their\n",
        "        termination critera are met.  In practice they have been used mainly in\n",
        "        discrete rather than in continuous optimization.\n",
        "        \n",
        "        Available annealing schedules are 'fast', 'cauchy' and 'boltzmann'.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized.  Must be in the form\n",
        "            `f(x, *args)`, where `x` is the argument in the form of a 1-D array\n",
        "            and `args` is a  tuple of any additional fixed parameters needed to\n",
        "            completely specify the function.\n",
        "        x0: 1-D array\n",
        "            An initial guess at the optimizing argument of `func`.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely\n",
        "            specify the objective function.\n",
        "        schedule : str, optional\n",
        "            The annealing schedule to use.  Must be one of 'fast', 'cauchy' or\n",
        "            'boltzmann'.  See `Notes`.\n",
        "        full_output : bool, optional\n",
        "            If `full_output`, then return all values listed in the Returns\n",
        "            section. Otherwise, return just the `xmin` and `status` values.\n",
        "        T0 : float, optional\n",
        "            The initial \"temperature\".  If None, then estimate it as 1.2 times\n",
        "            the largest cost-function deviation over random points in the\n",
        "            box-shaped region specified by the `lower, upper` input parameters.\n",
        "        Tf : float, optional\n",
        "            Final goal temperature.  Cease iterations if the temperature\n",
        "            falls below `Tf`.\n",
        "        maxeval : int, optional\n",
        "            Cease iterations if the number of function evaluations exceeds\n",
        "            `maxeval`.\n",
        "        maxaccept : int, optional\n",
        "            Cease iterations if the number of points accepted exceeds `maxaccept`.\n",
        "            See `Notes` for the probabilistic acceptance criteria used.\n",
        "        maxiter : int, optional\n",
        "            Cease iterations if the number of cooling iterations exceeds `maxiter`.\n",
        "        learn_rate : float, optional\n",
        "            Scale constant for tuning the probabilistc acceptance criteria.\n",
        "        boltzmann : float, optional\n",
        "            Boltzmann constant in the probabilistic acceptance criteria\n",
        "            (increase for less stringent criteria at each temperature).\n",
        "        feps : float, optional\n",
        "            Cease iterations if the relative errors in the function value over the\n",
        "            last four coolings is below `feps`.\n",
        "        quench, m, n : floats, optional\n",
        "            Parameters to alter the `fast` simulated annealing schedule.\n",
        "            See `Notes`.\n",
        "        lower, upper : floats or 1-D arrays, optional\n",
        "            Lower and upper bounds on the argument `x`.  If floats are provided,\n",
        "            they apply to all components of `x`.\n",
        "        dwell : int, optional\n",
        "            The number of times to execute the inner loop at each value of the\n",
        "            temperature.  See `Notes`.\n",
        "        disp : bool, optional\n",
        "            Print a descriptive convergence message if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            The point where the lowest function value was found.\n",
        "        Jmin : float\n",
        "            The objective function value at `xmin`.\n",
        "        T : float\n",
        "            The temperature at termination of the iterations.\n",
        "        feval : int\n",
        "            Number of function evaluations used.\n",
        "        iters : int\n",
        "            Number of cooling iterations used.\n",
        "        accept : int\n",
        "            Number of tests accepted.\n",
        "        status : int\n",
        "            A code indicating the reason for termination:\n",
        "        \n",
        "            - 0 : Points no longer changing.\n",
        "            - 1 : Cooled to final temperature.\n",
        "            - 2 : Maximum function evaluations reached.\n",
        "            - 3 : Maximum cooling iterations reached.\n",
        "            - 4 : Maximum accepted query locations reached.\n",
        "            - 5 : Final point not the minimum amongst encountered points.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        basinhopping : another (more performant) global optimizer\n",
        "        brute : brute-force global optimizer\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Simulated annealing is a random algorithm which uses no derivative\n",
        "        information from the function being optimized. In practice it has\n",
        "        been more useful in discrete optimization than continuous\n",
        "        optimization, as there are usually better algorithms for continuous\n",
        "        optimization problems.\n",
        "        \n",
        "        Some experimentation by trying the different temperature\n",
        "        schedules and altering their parameters is likely required to\n",
        "        obtain good performance.\n",
        "        \n",
        "        The randomness in the algorithm comes from random sampling in numpy.\n",
        "        To obtain the same results you can call `numpy.random.seed` with the\n",
        "        same seed immediately before calling `anneal`.\n",
        "        \n",
        "        We give a brief description of how the three temperature schedules\n",
        "        generate new points and vary their temperature.  Temperatures are\n",
        "        only updated with iterations in the outer loop.  The inner loop is\n",
        "        over loop over ``xrange(dwell)``, and new points are generated for\n",
        "        every iteration in the inner loop.  Whether the proposed new points\n",
        "        are accepted is probabilistic.\n",
        "        \n",
        "        For readability, let ``d`` denote the dimension of the inputs to func.\n",
        "        Also, let ``x_old`` denote the previous state, and ``k`` denote the\n",
        "        iteration number of the outer loop.  All other variables not\n",
        "        defined below are input variables to `anneal` itself.\n",
        "        \n",
        "        In the 'fast' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(0, 1, size = d)\n",
        "            y = sgn(u - 0.5) * T * ((1 + 1/T)**abs(2*u - 1) - 1.0)\n",
        "        \n",
        "            xc = y * (upper - lower)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            c = n * exp(-n * quench)\n",
        "            T_new = T0 * exp(-c * k**quench)\n",
        "        \n",
        "        In the 'cauchy' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(-pi/2, pi/2, size=d)\n",
        "            xc = learn_rate * T * tan(u)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            T_new = T0 / (1 + k)\n",
        "        \n",
        "        In the 'boltzmann' schedule the updates are::\n",
        "        \n",
        "            std = minimum(sqrt(T) * ones(d), (upper - lower) / (3*learn_rate))\n",
        "            y ~ Normal(0, std, size = d)\n",
        "            x_new = x_old + learn_rate * y\n",
        "        \n",
        "            T_new = T0 / log(1 + k)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        [1] P. J. M. van Laarhoven and E. H. L. Aarts, \"Simulated Annealing: Theory\n",
        "            and Applications\", Kluwer Academic Publishers, 1987.\n",
        "        \n",
        "        [2] W.H. Press et al., \"Numerical Recipies: The Art of Scientific Computing\",\n",
        "            Cambridge U. Press, 1987.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        *Example 1.* We illustrate the use of `anneal` to seek the global minimum\n",
        "        of a function of two variables that is equal to the sum of a positive-\n",
        "        definite quadratic and two deep \"Gaussian-shaped\" craters.  Specifically,\n",
        "        define the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``, ``params``, and the functions are\n",
        "        as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        >>> x0 = np.array([2., 2.])     # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(555)   # Seeded to allow replication.\n",
        "        >>> res = optimize.anneal(f, x0, args=params, schedule='boltzmann',\n",
        "                                  full_output=True, maxiter=500, lower=-10,\n",
        "                                  upper=10, dwell=250, disp=True)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res[0]  # obtained minimum\n",
        "        array([-1.03914194,  1.81330654])\n",
        "        >>> res[1]  # function value at minimum\n",
        "        -3.3817...\n",
        "        \n",
        "        So this run settled on the point [-1.039, 1.813] with a minimum function\n",
        "        value of about -3.382.  The final temperature was about 212. The run used\n",
        "        125301 function evaluations, 501 iterations (including the initial guess as\n",
        "        a iteration), and accepted 61162 points. The status flag of 3 also\n",
        "        indicates that `maxiter` was reached.\n",
        "        \n",
        "        This problem's true global minimum lies near the point [-1.057, 1.808]\n",
        "        and has a value of about -3.409.  So these `anneal` results are pretty\n",
        "        good and could be used as the starting guess in a local optimizer to\n",
        "        seek a more exact local minimum.\n",
        "        \n",
        "        *Example 2.* To minimize the same objective function using\n",
        "        the `minimize` approach, we need to (a) convert the options to an\n",
        "        \"options dictionary\" using the keys prescribed for this method,\n",
        "        (b) call the `minimize` function with the name of the method (which\n",
        "        in this case is 'Anneal'), and (c) take account of the fact that\n",
        "        the returned value will be a `Result` object (`i.e.`, a dictionary,\n",
        "        as defined in `optimize.py`).\n",
        "        \n",
        "        All of the allowable options for 'Anneal' when using the `minimize`\n",
        "        approach are listed in the ``myopts`` dictionary given below, although\n",
        "        in practice only the non-default values would be needed.  Some of their\n",
        "        names differ from those used in the `anneal` approach.  We can proceed\n",
        "        as follows:\n",
        "        \n",
        "        >>> myopts = {\n",
        "                'schedule'     : 'boltzmann',   # Non-default value.\n",
        "                'maxfev'       : None,  # Default, formerly `maxeval`.\n",
        "                'maxiter'      : 500,   # Non-default value.\n",
        "                'maxaccept'    : None,  # Default value.\n",
        "                'ftol'         : 1e-6,  # Default, formerly `feps`.\n",
        "                'T0'           : None,  # Default value.\n",
        "                'Tf'           : 1e-12, # Default value.\n",
        "                'boltzmann'    : 1.0,   # Default value.\n",
        "                'learn_rate'   : 0.5,   # Default value.\n",
        "                'quench'       : 1.0,   # Default value.\n",
        "                'm'            : 1.0,   # Default value.\n",
        "                'n'            : 1.0,   # Default value.\n",
        "                'lower'        : -10,   # Non-default value.\n",
        "                'upper'        : +10,   # Non-default value.\n",
        "                'dwell'        : 250,   # Non-default value.\n",
        "                'disp'         : True   # Default value.\n",
        "                }\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(777)  # Seeded to allow replication.\n",
        "        >>> res2 = optimize.minimize(f, x0, args=params, method='Anneal',\n",
        "                                     options=myopts)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res2\n",
        "          status: 3\n",
        "         success: False\n",
        "          accept: 61742\n",
        "            nfev: 125301\n",
        "               T: 214.20624873839623\n",
        "             fun: -3.4084065576676053\n",
        "               x: array([-1.05757366,  1.8071427 ])\n",
        "         message: 'Maximum cooling iterations reached'\n",
        "         nit: 501\n",
        "    \n",
        "    approx_fprime(xk, f, epsilon, *args)\n",
        "        Finite-difference approximation of the gradient of a scalar function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        xk : array_like\n",
        "            The coordinate vector at which to determine the gradient of `f`.\n",
        "        f : callable\n",
        "            The function of which to determine the gradient (partial derivatives).\n",
        "            Should take `xk` as first argument, other arguments to `f` can be\n",
        "            supplied in ``*args``.  Should return a scalar, the value of the\n",
        "            function at `xk`.\n",
        "        epsilon : array_like\n",
        "            Increment to `xk` to use for determining the function gradient.\n",
        "            If a scalar, uses the same finite difference delta for all partial\n",
        "            derivatives.  If an array, should contain one value per element of\n",
        "            `xk`.\n",
        "        \\*args : args, optional\n",
        "            Any other arguments that are to be passed to `f`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        grad : ndarray\n",
        "            The partial derivatives of `f` to `xk`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        check_grad : Check correctness of gradient function against approx_fprime.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The function gradient is determined by the forward finite difference\n",
        "        formula::\n",
        "        \n",
        "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
        "            f'[i] = ---------------------------------\n",
        "                                epsilon[i]\n",
        "        \n",
        "        The main use of `approx_fprime` is in scalar function optimizers like\n",
        "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c0, c1):\n",
        "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
        "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
        "        \n",
        "        >>> x = np.ones(2)\n",
        "        >>> c0, c1 = (1, 200)\n",
        "        >>> eps = np.sqrt(np.finfo(np.float).eps)\n",
        "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
        "        array([   2.        ,  400.00004198])\n",
        "    \n",
        "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None)\n",
        "        Find the global minimum of a function using the basin-hopping algorithm\n",
        "        \n",
        "        .. versionadded:: 0.12.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            Function to be optimized.  ``args`` can be passed as an optional item\n",
        "            in the dict ``minimizer_kwargs``\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        niter : integer, optional\n",
        "            The number of basin hopping iterations\n",
        "        T : float, optional\n",
        "            The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
        "            \"temperatures\" mean that larger jumps in function value will be\n",
        "            accepted.  For best results ``T`` should be comparable to the\n",
        "            separation\n",
        "            (in function value) between local minima.\n",
        "        stepsize : float, optional\n",
        "            initial step size for use in the random displacement.\n",
        "        minimizer_kwargs : dict, optional\n",
        "            Extra keyword arguments to be passed to the minimizer\n",
        "            ``scipy.optimize.minimize()`` Some important options could be:\n",
        "                method : str\n",
        "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
        "                args : tuple\n",
        "                    Extra arguments passed to the objective function (``func``) and\n",
        "                    its derivatives (Jacobian, Hessian).\n",
        "        \n",
        "        take_step : callable ``take_step(x)``, optional\n",
        "            Replace the default step taking routine with this routine.  The default\n",
        "            step taking routine is a random displacement of the coordinates, but\n",
        "            other step taking algorithms may be better for some systems.\n",
        "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
        "            If this attribute exists, then ``basinhopping`` will adjust\n",
        "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
        "            search.\n",
        "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
        "            Define a test which will be used to judge whether or not to accept the\n",
        "            step.  This will be used in addition to the Metropolis test based on\n",
        "            \"temperature\" ``T``.  The acceptable return values are True,\n",
        "            False, or ``\"force accept\"``.  If the latter, then this will\n",
        "            override any other tests in order to accept the step.  This can be\n",
        "            used, for example, to forcefully escape from a local minimum that\n",
        "            ``basinhopping`` is trapped in.\n",
        "        callback : callable, ``callback(x, f, accept)``, optional\n",
        "            A callback function which will be called for all minimum found.  ``x``\n",
        "            and ``f`` are the coordinates and function value of the trial minima,\n",
        "            and ``accept`` is whether or not that minima was accepted.  This can be\n",
        "            used, for example, to save the lowest N minima found.  Also,\n",
        "            ``callback`` can be used to specify a user defined stop criterion by\n",
        "            optionally returning True to stop the ``basinhopping`` routine.\n",
        "        interval : integer, optional\n",
        "            interval for how often to update the ``stepsize``\n",
        "        disp : bool, optional\n",
        "            Set to True to print status messages\n",
        "        niter_success : integer, optional\n",
        "            Stop the run if the global minimum candidate remains the same for this\n",
        "            number of iterations.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.  Important\n",
        "            attributes are: ``x`` the solution array, ``fun`` the value of the\n",
        "            function at the solution, and ``message`` which describes the cause of\n",
        "            the termination. See `Result` for a description of other attributes.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize :\n",
        "            The local minimization function called once for each basinhopping step.\n",
        "            ``minimizer_kwargs`` is passed to this routine.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
        "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
        "        [4]_.  The algorithm in its current form was described by David Wales and\n",
        "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
        "        \n",
        "        The algorithm is iterative with each cycle composed of the following\n",
        "        features\n",
        "        \n",
        "        1) random perturbation of the coordinates\n",
        "        \n",
        "        2) local minimization\n",
        "        \n",
        "        3) accept or reject the new coordinates based on the minimized function\n",
        "           value\n",
        "        \n",
        "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
        "        Carlo algorithms, although there are many other possibilities [3]_.\n",
        "        \n",
        "        This global minimization method has been shown to be extremely efficient\n",
        "        for a wide variety of problems in physics and chemistry.  It is\n",
        "        particularly useful when the function has many minima separated by large\n",
        "        barriers. See the Cambridge Cluster Database\n",
        "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
        "        that have been optimized primarily using basin-hopping.  This database\n",
        "        includes minimization problems exceeding 300 degrees of freedom.\n",
        "        \n",
        "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
        "        a Fortran implementation of basin-hopping.  This implementation has many\n",
        "        different variations of the procedure described above, including more\n",
        "        advanced step taking algorithms and alternate acceptance criterion.\n",
        "        \n",
        "        For stochastic global optimization there is no way to determine if the true\n",
        "        global minimum has actually been found. Instead, as a consistency check,\n",
        "        the algorithm can be run from a number of different random starting points\n",
        "        to ensure the lowest minimum found in each example has converged to the\n",
        "        global minimum.  For this reason ``basinhopping`` will by default simply\n",
        "        run for the number of iterations ``niter`` and return the lowest minimum\n",
        "        found.  It is left to the user to ensure that this is in fact the global\n",
        "        minimum.\n",
        "        \n",
        "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
        "        depends on the problem being solved.  Ideally it should be comparable to\n",
        "        the typical separation between local minima of the function being\n",
        "        optimized.  ``basinhopping`` will, by default, adjust ``stepsize`` to find\n",
        "        an optimal value, but this may take many iterations.  You will get quicker\n",
        "        results if you set a sensible value for ``stepsize``.\n",
        "        \n",
        "        Choosing ``T``: The parameter ``T`` is the temperature used in the\n",
        "        metropolis criterion.  Basinhopping steps are accepted with probability\n",
        "        ``1`` if ``func(xnew) < func(xold)``, or otherwise with probability::\n",
        "        \n",
        "            exp( -(func(xnew) - func(xold)) / T )\n",
        "        \n",
        "        So, for best results, ``T`` should to be comparable to the typical\n",
        "        difference in function value between between local minima\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
        "            Cambridge, UK.\n",
        "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
        "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
        "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
        "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
        "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
        "            1987, 84, 6611.\n",
        "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
        "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following example is a one-dimensional minimization problem,  with many\n",
        "        local minima superimposed on a parabola.\n",
        "        \n",
        "        >>> func = lambda x: cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
        "        >>> x0=[1.]\n",
        "        \n",
        "        Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
        "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
        "        use and how to set up that minimizer.  This parameter will be passed to\n",
        "        ``scipy.optimize.minimize()``.\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
        "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
        "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
        "        \n",
        "        Next consider a two-dimensional minimization problem. Also, this time we\n",
        "        will use gradient information to significantly speed up the search.\n",
        "        \n",
        "        >>> def func2d(x):\n",
        "        ...     f = cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
        "        ...                                                         0.2) * x[0]\n",
        "        ...     df = np.zeros(2)\n",
        "        ...     df[0] = -14.5 * sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
        "        ...     df[1] = 2. * x[1] + 0.2\n",
        "        ...     return f, df\n",
        "        \n",
        "        We'll also use a different local minimization algorithm.  Also we must tell\n",
        "        the minimizer that our function returns both energy and gradient (jacobian)\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
        "        >>> x0 = [1.0, 1.0]\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Here is an example using a custom step taking routine.  Imagine you want\n",
        "        the first coordinate to take larger steps then the rest of the coordinates.\n",
        "        This can be implemented like so:\n",
        "        \n",
        "        >>> class MyTakeStep(object):\n",
        "        ...    def __init__(self, stepsize=0.5):\n",
        "        ...        self.stepsize = stepsize\n",
        "        ...    def __call__(self, x):\n",
        "        ...        s = self.stepsize\n",
        "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
        "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
        "        ...        return x\n",
        "        \n",
        "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
        "        of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
        "        before\n",
        "        \n",
        "        >>> mytakestep = MyTakeStep()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200, take_step=mytakestep)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Now let's do an example using a custom callback function which prints the\n",
        "        value of every minimum found\n",
        "        \n",
        "        >>> def print_fun(x, f, accepted):\n",
        "        ...         print(\"at minima %.4f accepted %d\" % (f, int(accepted)))\n",
        "        \n",
        "        We'll run it for only 10 basinhopping steps this time.\n",
        "        \n",
        "        >>> np.random.seed(1)\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, callback=print_fun)\n",
        "        at minima 0.4159 accepted 1\n",
        "        at minima -0.9073 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 2.2945 accepted 0\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        \n",
        "        \n",
        "        The minima at -1.0109 is actually the global minimum, found already on the\n",
        "        8th iteration.\n",
        "        \n",
        "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
        "        \n",
        "        >>> class MyBounds(object):\n",
        "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
        "        ...         self.xmax = np.array(xmax)\n",
        "        ...         self.xmin = np.array(xmin)\n",
        "        ...     def __call__(self, **kwargs):\n",
        "        ...         x = kwargs[\"x_new\"]\n",
        "        ...         tmax = bool(np.all(x <= self.xmax))\n",
        "        ...         tmin = bool(np.all(x >= self.xmin))\n",
        "        ...         return tmax and tmin\n",
        "        \n",
        "        >>> mybounds = MyBounds()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, accept_test=mybounds)\n",
        "    \n",
        "    bisect(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of a function within an interval.\n",
        "        \n",
        "        Basic bisection routine to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. `f(a)` and `f(b)` can not have the same signs.\n",
        "        Slow but sure.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  `f` must be continuous, and\n",
        "            f(a) and f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within `xtol` of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in `maxiter` iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
        "            a `RootResults` object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        fsolve : n-dimensional root-finding\n",
        "    \n",
        "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
        "        Bracket the minimum of the function.\n",
        "        \n",
        "        Given a function and distinct initial points, search in the\n",
        "        downhill direction (as defined by the initital points) and return\n",
        "        new points xa, xb, xc that bracket the minimum of the function\n",
        "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
        "        solution will satisfy xa<=x<=xb\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to minimize.\n",
        "        xa, xb : float, optional\n",
        "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
        "        args : tuple, optional\n",
        "            Additional arguments (if present), passed to `func`.\n",
        "        grow_limit : float, optional\n",
        "            Maximum grow limit.  Defaults to 110.0\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Defaults to 1000.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xa, xb, xc : float\n",
        "            Bracket.\n",
        "        fa, fb, fc : float\n",
        "            Objective function values in bracket.\n",
        "        funcalls : int\n",
        "            Number of function evaluations made.\n",
        "    \n",
        "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
        "        Given a function of one-variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        args\n",
        "            Additional arguments (if present).\n",
        "        brack : tuple\n",
        "            Triple (a,b,c) where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,c)\n",
        "            then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that the obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            Stop if between iteration change is less than `tol`.\n",
        "        full_output : bool\n",
        "            If True, return all output args (xmin, fval, iter,\n",
        "            funcalls).\n",
        "        maxiter : int\n",
        "            Maximum number of iterations in solution.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            Optimum point.\n",
        "        fval : float\n",
        "            Optimum value.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of objective function evaluations made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Brent' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses inverse parabolic interpolation when possible to speed up\n",
        "        convergence of golden section method.\n",
        "    \n",
        "    brenth(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of f in [a,b].\n",
        "        \n",
        "        A variation on the classic Brent routine to find a zero of the function f\n",
        "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
        "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
        "        f(a) and f(b) can not have the same signs. Generally on a par with the\n",
        "        brent routine, but not as heavily tested.  It is a safe version of the\n",
        "        secant method that uses hyperbolic extrapolation. The version here is by\n",
        "        Chuck Harris.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        fmin, fmin_powell, fmin_cg,\n",
        "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
        "        \n",
        "        leastsq : nonlinear least squares minimizer\n",
        "        \n",
        "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
        "        \n",
        "        anneal, brute : global optimizers\n",
        "        \n",
        "        fminbound, brent, golden, bracket : local scalar minimizers\n",
        "        \n",
        "        fsolve : n-dimensional root-finding\n",
        "        \n",
        "        brentq, brenth, ridder, bisect, newton : one-dimensional root-finding\n",
        "        \n",
        "        fixed_point : scalar fixed-point finder\n",
        "    \n",
        "    brentq(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in given interval.\n",
        "        \n",
        "        Return float, a zero of `f` between `a` and `b`.  `f` must be a continuous\n",
        "        function, and [a,b] must be a sign changing interval.\n",
        "        \n",
        "        Description:\n",
        "        Uses the classic Brent (1973) method to find a zero of the function `f` on\n",
        "        the sign changing interval [a , b].  Generally considered the best of the\n",
        "        rootfinding routines here.  It is a safe version of the secant method that\n",
        "        uses inverse quadratic extrapolation.  Brent's method combines root\n",
        "        bracketing, interval bisection, and inverse quadratic interpolation.  It is\n",
        "        sometimes known as the van Wijngaarden-Deker-Brent method.  Brent (1973)\n",
        "        claims convergence is guaranteed for functions computable within [a,b].\n",
        "        \n",
        "        [Brent1973]_ provides the classic description of the algorithm.  Another\n",
        "        description can be found in a recent edition of Numerical Recipes, including\n",
        "        [PressEtal1992]_.  Another description is at\n",
        "        http://mathworld.wolfram.com/BrentsMethod.html.  It should be easy to\n",
        "        understand the algorithm just by reading our code.  Our code diverges a bit\n",
        "        from standard presentations: we choose a different formula for the\n",
        "        extrapolation step.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        multivariate local optimizers\n",
        "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
        "        nonlinear least squares minimizer\n",
        "          `leastsq`\n",
        "        constrained multivariate optimizers\n",
        "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
        "        global optimizers\n",
        "          `anneal`, `basinhopping`, `brute`\n",
        "        local scalar minimizers\n",
        "          `fminbound`, `brent`, `golden`, `bracket`\n",
        "        n-dimensional root-finding\n",
        "          `fsolve`\n",
        "        one-dimensional root-finding\n",
        "          `brentq`, `brenth`, `ridder`, `bisect`, `newton`\n",
        "        scalar fixed-point finder\n",
        "          `fixed_point`\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Brent1973]\n",
        "           Brent, R. P.,\n",
        "           *Algorithms for Minimization Without Derivatives*.\n",
        "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
        "        \n",
        "        .. [PressEtal1992]\n",
        "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
        "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
        "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
        "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
        "    \n",
        "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \\\"Broyden's good method\\\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
        "        \n",
        "        which corresponds to Broyden's first Jacobian update\n",
        "        \n",
        "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \\\"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \"Broyden's bad method\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
        "        \n",
        "        corresponding to Broyden's second method.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin>, disp=False)\n",
        "        Minimize a function over a given range by brute force.\n",
        "        \n",
        "        Uses the \"brute force\" method, i.e. computes the function's value\n",
        "        at each point of a multidimensional grid of points, to find the global\n",
        "        minimum of the function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized. Must be in the\n",
        "            form ``f(x, *args)``, where ``x`` is the argument in\n",
        "            the form of a 1-D array and ``args`` is a tuple of any\n",
        "            additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        ranges : tuple\n",
        "            Each component of the `ranges` tuple must be either a\n",
        "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
        "            The program uses these to create the grid of points on which\n",
        "            the objective function will be computed. See `Note 2` for\n",
        "            more detail.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        Ns : int, optional\n",
        "            Number of grid points along the axes, if not otherwise\n",
        "            specified. See `Note2`.\n",
        "        full_output : bool, optional\n",
        "            If True, return the evaluation grid and the objective function's\n",
        "            values on it.\n",
        "        finish : callable, optional\n",
        "            An optimization function that is called with the result of brute force\n",
        "            minimization as initial guess.  `finish` should take the initial guess\n",
        "            as positional argument, and take `args`, `full_output` and `disp`\n",
        "            as keyword arguments.  Use None if no \"polishing\" function is to be\n",
        "            used.  See Notes for more details.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : ndarray\n",
        "            A 1-D array containing the coordinates of a point at which the\n",
        "            objective function had its minimum value. (See `Note 1` for\n",
        "            which point is returned.)\n",
        "        fval : float\n",
        "            Function value at the point `x0`.\n",
        "        grid : tuple\n",
        "            Representation of the evaluation grid.  It has the same\n",
        "            length as `x0`. (Returned when `full_output` is True.)\n",
        "        Jout : ndarray\n",
        "            Function values at each point of the evaluation\n",
        "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
        "            when `full_output` is True.)\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        anneal : Another approach to seeking the global minimum of\n",
        "        multivariate, multimodal functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
        "        of the objective function occurs.  If `finish` is None, that is the\n",
        "        point returned.  When the global minimum occurs within (or not very far\n",
        "        outside) the grid's boundaries, and the grid is fine enough, that\n",
        "        point will be in the neighborhood of the gobal minimum.\n",
        "        \n",
        "        However, users often employ some other optimization program to\n",
        "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
        "        (local) minimum near `brute's` best gridpoint.\n",
        "        The `brute` function's `finish` option provides a convenient way to do\n",
        "        that.  Any polishing program used must take `brute's` output as its\n",
        "        initial guess as a positional argument, and take `brute's` input values\n",
        "        for `args` and `full_output` as keyword arguments, otherwise an error\n",
        "        will be raised.\n",
        "        \n",
        "        `brute` assumes that the `finish` function returns a tuple in the form:\n",
        "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing value\n",
        "        of the argument, ``Jmin`` is the minimum value of the objective function,\n",
        "        \"...\" may be some other returned values (which are not used by `brute`),\n",
        "        and ``statuscode`` is the status code of the `finish` program.\n",
        "        \n",
        "        Note that when `finish` is not None, the values returned are those\n",
        "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
        "        while `brute` confines its search to the input grid points,\n",
        "        the `finish` program's results usually will not coincide with any\n",
        "        gridpoint, and may fall outside the grid's boundary.\n",
        "        \n",
        "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
        "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
        "        Each component of the `ranges` tuple can be either a slice object or a\n",
        "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
        "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
        "        range, `brute` internally converts it to a slice object that interpolates\n",
        "        `Ns` points from its low-value to its high-value, inclusive.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        We illustrate the use of `brute` to seek the global minimum of a function\n",
        "        of two variables that is given as the sum of a positive-definite\n",
        "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
        "        the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
        "        are as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        Thus, the objective function may have local minima near the minimum\n",
        "        of each of the three functions of which it is composed.  To\n",
        "        use `fmin` to polish its gridpoint result, we may then continue as\n",
        "        follows:\n",
        "        \n",
        "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
        "        >>> from scipy import optimize\n",
        "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
        "                                      finish=optimize.fmin)\n",
        "        >>> resbrute[0]  # global minimum\n",
        "        array([-1.05665192,  1.80834843])\n",
        "        >>> resbrute[1]  # function value at global minimum\n",
        "        -3.4085818767\n",
        "        \n",
        "        Note that if `finish` had been set to None, we would have gotten the\n",
        "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
        "    \n",
        "    check_grad(func, grad, x0, *args)\n",
        "        Check the correctness of a gradient function by comparing it against a\n",
        "        (forward) finite-difference approximation of the gradient.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x0,*args)\n",
        "            Function whose derivative is to be checked.\n",
        "        grad : callable grad(x0, *args)\n",
        "            Gradient of `func`.\n",
        "        x0 : ndarray\n",
        "            Points to check `grad` against forward difference approximation of grad\n",
        "            using `func`.\n",
        "        args : \\*args, optional\n",
        "            Extra arguments passed to `func` and `grad`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        err : float\n",
        "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
        "            difference between ``grad(x0, *args)`` and the finite difference\n",
        "            approximation of `grad` using func at the points `x0`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        approx_fprime\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The step size used for the finite difference approximation is\n",
        "        `sqrt(numpy.finfo(float).eps)`, which is approximately 1.49e-08.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> def func(x): return x[0]**2 - 0.5 * x[1]**3\n",
        "        >>> def grad(x): return [2 * x[0], -1.5 * x[1]**2]\n",
        "        >>> check_grad(func, grad, [1.5, -1.5])\n",
        "        2.9802322387695312e-08\n",
        "    \n",
        "    curve_fit(f, xdata, ydata, p0=None, sigma=None, **kw)\n",
        "        Use non-linear least squares to fit a function, f, to data.\n",
        "        \n",
        "        Assumes ``ydata = f(xdata, *params) + eps``\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable\n",
        "            The model function, f(x, ...).  It must take the independent\n",
        "            variable as the first argument and the parameters to fit as\n",
        "            separate remaining arguments.\n",
        "        xdata : An N-length sequence or an (k,N)-shaped array\n",
        "            for functions with k predictors.\n",
        "            The independent variable where the data is measured.\n",
        "        ydata : N-length sequence\n",
        "            The dependent data --- nominally f(xdata, ...)\n",
        "        p0 : None, scalar, or M-length sequence\n",
        "            Initial guess for the parameters.  If None, then the initial\n",
        "            values will all be 1 (if the number of parameters for the function\n",
        "            can be determined using introspection, otherwise a ValueError\n",
        "            is raised).\n",
        "        sigma : None or N-length sequence\n",
        "            If not None, this vector will be used as relative weights in the\n",
        "            least-squares problem.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        popt : array\n",
        "            Optimal values for the parameters so that the sum of the squared error\n",
        "            of ``f(xdata, *popt) - ydata`` is minimized\n",
        "        pcov : 2d array\n",
        "            The estimated covariance of popt.  The diagonals provide the variance\n",
        "            of the parameter estimate.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        leastsq\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The algorithm uses the Levenberg-Marquardt algorithm through `leastsq`.\n",
        "        Additional keyword arguments are passed directly to that algorithm.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> import numpy as np\n",
        "        >>> from scipy.optimize import curve_fit\n",
        "        >>> def func(x, a, b, c):\n",
        "        ...     return a*np.exp(-b*x) + c\n",
        "        \n",
        "        >>> x = np.linspace(0,4,50)\n",
        "        >>> y = func(x, 2.5, 1.3, 0.5)\n",
        "        >>> yn = y + 0.2*np.random.normal(size=len(x))\n",
        "        \n",
        "        >>> popt, pcov = curve_fit(func, x, yn)\n",
        "    \n",
        "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
        "        \n",
        "        The Jacobian approximation is derived from previous iterations, by\n",
        "        retaining only the diagonal of Broyden matrices.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
        "        \n",
        "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial Jacobian approximation is (-1/alpha).\n",
        "        alphamax : float, optional\n",
        "            The entries of the diagonal Jacobian are kept in the range\n",
        "            ``[alpha, alphamax]``.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500)\n",
        "        Find a fixed point of the function.\n",
        "        \n",
        "        Given a function of one or more variables and a starting point, find a\n",
        "        fixed-point of the function: i.e. where ``func(x0) == x0``.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            Function to evaluate.\n",
        "        x0 : array_like\n",
        "            Fixed point of function.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to `func`.\n",
        "        xtol : float, optional\n",
        "            Convergence tolerance, defaults to 1e-08.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations, defaults to 500.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses Steffensen's Method using Aitken's ``Del^2`` convergence acceleration.\n",
        "        See Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c1, c2):\n",
        "        ....    return np.sqrt(c1/(x+c2))\n",
        "        >>> c1 = np.array([10,12.])\n",
        "        >>> c2 = np.array([3, 5.])\n",
        "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
        "        array([ 1.4920333 ,  1.37228132])\n",
        "    \n",
        "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the downhill simplex algorithm.\n",
        "        \n",
        "        This algorithm only uses function values, not derivatives or second\n",
        "        derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            The objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        xtol : float, optional\n",
        "            Relative error in xopt acceptable for convergence.\n",
        "        ftol : number, optional\n",
        "            Relative error in func(xopt) acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : number, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            Set to True if fopt and warnflag outputs are desired.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        retall : bool, optional\n",
        "            Set to True to return list of solutions at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter that minimizes function.\n",
        "        fopt : float\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        iter : int\n",
        "            Number of iterations performed.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            1 : Maximum number of function evaluations made.\n",
        "            2 : Maximum number of iterations reached.\n",
        "        allvecs : list\n",
        "            Solution at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Nelder-Mead' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "        one or more variables.\n",
        "        \n",
        "        This algorithm has a long history of successful use in applications.\n",
        "        But it will usually be slower than an algorithm that uses first or\n",
        "        second derivative information. In practice it can have poor\n",
        "        performance in high-dimensional problems and is not robust to\n",
        "        minimizing complicated functions. Additionally, there currently is no\n",
        "        complete theory describing when the algorithm will successfully\n",
        "        converge to the minimum, or how fast it will if it does.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "               minimization\", The Computer Journal, 7, pp. 308-313\n",
        "        \n",
        "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "               Harlow, UK, pp. 191-208.\n",
        "    \n",
        "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the BFGS algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable f'(x,*args), optional\n",
        "            Gradient of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f and fprime.\n",
        "        gtol : float, optional\n",
        "            Gradient norm must be less than gtol before succesful termination.\n",
        "        norm : float, optional\n",
        "            Order of norm (Inf is max, -Inf is min)\n",
        "        epsilon : int or ndarray, optional\n",
        "            If fprime is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function to call after each\n",
        "            iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
        "            in addition to xopt.\n",
        "        disp : bool, optional\n",
        "            Print convergence message if True.\n",
        "        retall : bool, optional\n",
        "            Return a list of results at each iteration if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
        "        fopt : float\n",
        "            Minimum value.\n",
        "        gopt : ndarray\n",
        "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
        "        Bopt : ndarray\n",
        "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
        "        func_calls : int\n",
        "            Number of function_calls made.\n",
        "        grad_calls : int\n",
        "            Number of gradient calls made.\n",
        "        warnflag : integer\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "            2 : Gradient and/or function calls not changing.\n",
        "        allvecs  :  list\n",
        "            Results at each iteration.  Only returned if retall is True.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'BFGS' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Optimize the function, f, whose gradient is given by fprime\n",
        "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
        "        and Shanno (BFGS)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
        "    \n",
        "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable, ``f(x, *args)``\n",
        "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
        "            the variables that are to be changed in the search for a minimum, and\n",
        "            `args` are the other (fixed) parameters of `f`.\n",
        "        x0 : ndarray\n",
        "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
        "            It must be a 1-D array of values.\n",
        "        fprime : callable, ``fprime(x, *args)``, optional\n",
        "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
        "            are as described above for `f`. The returned value must be a 1-D array.\n",
        "            Defaults to None, in which case the gradient is approximated\n",
        "            numerically (see `epsilon`, below).\n",
        "        args : tuple, optional\n",
        "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
        "            additional fixed parameters are needed to completely specify the\n",
        "            functions `f` and `fprime`.\n",
        "        gtol : float, optional\n",
        "            Stop when the norm of the gradient is less than `gtol`.\n",
        "        norm : float, optional\n",
        "            Order to use for the norm of the gradient\n",
        "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
        "        epsilon : float or ndarray, optional\n",
        "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
        "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
        "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
        "            1.5e-8.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
        "        full_output : bool, optional\n",
        "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
        "            addition to `xopt`.  See the Returns section below for additional\n",
        "            information on optional return values.\n",
        "        disp : bool, optional\n",
        "            If True, return a convergence message, followed by `xopt`.\n",
        "        retall : bool, optional\n",
        "            If True, add to the returned values the results of each iteration.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each iteration.\n",
        "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float, optional\n",
        "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
        "        func_calls : int, optional\n",
        "            The number of function_calls made.  Only returned if `full_output`\n",
        "            is True.\n",
        "        grad_calls : int, optional\n",
        "            The number of gradient calls made. Only returned if `full_output` is\n",
        "            True.\n",
        "        warnflag : int, optional\n",
        "            Integer value with warning status, only returned if `full_output` is\n",
        "            True.\n",
        "        \n",
        "            0 : Success.\n",
        "        \n",
        "            1 : The maximum number of iterations was exceeded.\n",
        "        \n",
        "            2 : Gradient and/or function calls were not changing.  May indicate\n",
        "                that precision was lost, i.e., the routine did not converge.\n",
        "        \n",
        "        allvecs : list of ndarray, optional\n",
        "            List of arrays, containing the results at each iteration.\n",
        "            Only returned if `retall` is True.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize : common interface to all `scipy.optimize` algorithms for\n",
        "                   unconstrained and constrained minimization of multivariate\n",
        "                   functions.  It provides an alternative way to call\n",
        "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
        "        [1]_.\n",
        "        \n",
        "        Conjugate gradient methods tend to work better when:\n",
        "        \n",
        "        1. `f` has a unique global minimizing point, and no local minima or\n",
        "           other stationary points,\n",
        "        2. `f` is, at least locally, reasonably well approximated by a\n",
        "           quadratic function of the variables,\n",
        "        3. `f` is continuous and has a continuous gradient,\n",
        "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
        "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
        "           minimizing point, `xopt`.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Example 1: seek the minimum value of the expression\n",
        "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
        "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
        "        \n",
        "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
        "        >>> def f(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
        "        >>> def gradf(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
        "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
        "        ...     return np.asarray((gu, gv))\n",
        "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
        "        >>> print 'res1 = ', res1\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 1.617021\n",
        "                 Iterations: 2\n",
        "                 Function evaluations: 5\n",
        "                 Gradient evaluations: 5\n",
        "        res1 =  [-1.80851064 -0.25531915]\n",
        "        \n",
        "        Example 2: solve the same problem using the `minimize` function.\n",
        "        (This `myopts` dictionary shows all of the available options,\n",
        "        although in practice only non-default values would be needed.\n",
        "        The returned value will be a dictionary.)\n",
        "        \n",
        "        >>> opts = {'maxiter' : None,    # default value.\n",
        "        ...         'disp' : True,    # non-default value.\n",
        "        ...         'gtol' : 1e-5,    # default value.\n",
        "        ...         'norm' : np.inf,  # default value.\n",
        "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
        "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
        "        ...                          method='CG', options=opts)\n",
        "        Optimization terminated successfully.\n",
        "                Current function value: 1.617021\n",
        "                Iterations: 2\n",
        "                Function evaluations: 5\n",
        "                Gradient evaluations: 5\n",
        "        >>> res2.x  # minimum found\n",
        "        array([-1.80851064 -0.25531915])\n",
        "    \n",
        "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, iprint=1, maxfun=1000, disp=None)\n",
        "        Minimize a function using the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
        "        implentation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            Function to minimize. In the form func(x, \\*args).\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        cons : sequence\n",
        "            Constraint functions; must all be ``>=0`` (a single function\n",
        "            if only 1 constraint). Each function takes the parameters `x`\n",
        "            as its first argument.\n",
        "        args : tuple\n",
        "            Extra arguments to pass to function.\n",
        "        consargs : tuple\n",
        "            Extra arguments to pass to constraint functions (default of None means\n",
        "            use same extra arguments as those passed to func).\n",
        "            Use ``()`` for no extra arguments.\n",
        "        rhobeg :\n",
        "            Reasonable initial changes to the variables.\n",
        "        rhoend :\n",
        "            Final accuracy in the optimization (not precisely guaranteed). This\n",
        "            is a lower bound on the size of the trust region.\n",
        "        iprint : {0, 1, 2, 3}\n",
        "            Controls the frequency of output; 0 implies no output.  Deprecated.\n",
        "        disp : {0, 1, 2, 3}\n",
        "            Over-rides the iprint interface.  Preferred.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The argument that minimises `f`.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'COBYLA' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm is based on linear approximations to the objective\n",
        "        function and each constraint. We briefly describe the algorithm.\n",
        "        \n",
        "        Suppose the function is being minimized over k variables. At the\n",
        "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
        "        an approximate solution x_j, and a radius RHO_j.\n",
        "        (i.e. linear plus a constant) approximations to the objective\n",
        "        function and constraint functions such that their function values\n",
        "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
        "        This gives a linear program to solve (where the linear approximations\n",
        "        of the constraint functions are constrained to be non-negative).\n",
        "        \n",
        "        However the linear approximations are likely only good\n",
        "        approximations near the current simplex, so the linear program is\n",
        "        given the further requirement that the solution, which\n",
        "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
        "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
        "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
        "        like a trust region algorithm.\n",
        "        \n",
        "        Additionally, the linear program may be inconsistent, or the\n",
        "        approximation may give poor improvement. For details about\n",
        "        how these issues are resolved, as well as how the points v_i are\n",
        "        updated, refer to the source code or the references below.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
        "        the objective and constraint functions by linear interpolation.\", in\n",
        "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
        "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
        "        \n",
        "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
        "        calculations\", Acta Numerica 7, 287-336\n",
        "        \n",
        "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
        "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
        "        \n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Minimize the objective function f(x,y) = x*y subject\n",
        "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
        "        \n",
        "            >>> def objective(x):\n",
        "            ...     return x[0]*x[1]\n",
        "            ...\n",
        "            >>> def constr1(x):\n",
        "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
        "            ...\n",
        "            >>> def constr2(x):\n",
        "            ...     return x[1]\n",
        "            ...\n",
        "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
        "        \n",
        "               Normal return from subroutine COBYLA\n",
        "        \n",
        "               NFVALS =   64   F =-5.000000E-01    MAXCV = 1.998401E-14\n",
        "               X =-7.071069E-01   7.071067E-01\n",
        "            array([-0.70710685,  0.70710671])\n",
        "        \n",
        "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
        "    \n",
        "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None)\n",
        "        Minimize a function func using the L-BFGS-B algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Function to minimise.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable fprime(x,*args)\n",
        "            The gradient of `func`.  If None, then `func` returns the function\n",
        "            value and the gradient (``f, g = func(x, *args)``), unless\n",
        "            `approx_grad` is True in which case `func` returns only ``f``.\n",
        "        args : sequence\n",
        "            Arguments to pass to `func` and `fprime`.\n",
        "        approx_grad : bool\n",
        "            Whether to approximate the gradient numerically (in which case\n",
        "            `func` returns only the function value).\n",
        "        bounds : list\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        m : int\n",
        "            The maximum number of variable metric corrections\n",
        "            used to define the limited memory matrix. (The limited memory BFGS\n",
        "            method does not store the full hessian but uses this many terms in an\n",
        "            approximation to it.)\n",
        "        factr : float\n",
        "            The iteration stops when\n",
        "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
        "            where ``eps`` is the machine precision, which is automatically\n",
        "            generated by the code. Typical values for `factr` are: 1e12 for\n",
        "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
        "            high accuracy.\n",
        "        pgtol : float\n",
        "            The iteration will stop when\n",
        "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
        "            where ``pg_i`` is the i-th component of the projected gradient.\n",
        "        epsilon : float\n",
        "            Step size used when `approx_grad` is True, for numerically\n",
        "            calculating the gradient\n",
        "        iprint : int\n",
        "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
        "            ``iprint == 0`` means write messages to stdout; ``iprint > 1`` in\n",
        "            addition means write logging information to a file named\n",
        "            ``iterate.dat`` in the current working directory.\n",
        "        disp : int, optional\n",
        "            If zero, then no output.  If a positive number, then this over-rides\n",
        "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        maxiter : int\n",
        "            Maximum number of iterations.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : array_like\n",
        "            Estimated position of the minimum.\n",
        "        f : float\n",
        "            Value of `func` at the minimum.\n",
        "        d : dict\n",
        "            Information dictionary.\n",
        "        \n",
        "            * d['warnflag'] is\n",
        "        \n",
        "              - 0 if converged,\n",
        "              - 1 if too many function evaluations or too many iterations,\n",
        "              - 2 if stopped for another reason, given in d['task']\n",
        "        \n",
        "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
        "            * d['funcalls'] is the number of function calls made.\n",
        "            * d['nit'] is the number of iterations.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'L-BFGS-B' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        License of L-BFGS-B (FORTRAN code):\n",
        "        \n",
        "        The version included here (in fortran code) is 3.0\n",
        "        (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd,\n",
        "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
        "        condition for use:\n",
        "        \n",
        "        This software is freely available, but we expect that all publications\n",
        "        describing work using this software, or all commercial products using it,\n",
        "        quote at least one of the references given below. This software is released\n",
        "        under the BSD License.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
        "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
        "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
        "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
        "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
        "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
        "          ACM Transactions on Mathematical Software, 38, 1.\n",
        "    \n",
        "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Unconstrained minimization of a function using the Newton-CG method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable ``f(x, *args)``\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable ``f'(x, *args)``\n",
        "            Gradient of f.\n",
        "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
        "            Function which computes the Hessian of f times an\n",
        "            arbitrary vector, p.\n",
        "        fhess : callable ``fhess(x, *args)``, optional\n",
        "            Function to compute the Hessian matrix of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
        "            (the same set of extra arguments is supplied to all of\n",
        "            these functions).\n",
        "        epsilon : float or ndarray, optional\n",
        "            If fhess is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function which is called after\n",
        "            each iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        avextol : float, optional\n",
        "            Convergence is assumed when the average relative error in\n",
        "            the minimizer falls below this amount.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True, return the optional outputs.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence message.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of results at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float\n",
        "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
        "        fcalls : int\n",
        "            Number of function calls made.\n",
        "        gcalls : int\n",
        "            Number of gradient calls made.\n",
        "        hcalls : int\n",
        "            Number of hessian calls made.\n",
        "        warnflag : int\n",
        "            Warnings generated by the algorithm.\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "        allvecs : list\n",
        "            The result at each iteration, if retall is True (see below).\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Newton-CG' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
        "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
        "        nor `fhess_p` is provided, then the hessian product will be\n",
        "        approximated using finite differences on `fprime`. `fhess_p`\n",
        "        must compute the hessian times an arbitrary vector. If it is not\n",
        "        given, finite-differences on `fprime` are used to compute\n",
        "        it.\n",
        "        \n",
        "        Newton-CG methods are also called truncated Newton methods. This\n",
        "        function differs from scipy.optimize.fmin_tnc because\n",
        "        \n",
        "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
        "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
        "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
        "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
        "            or box constrained minimization. (Box constraints give\n",
        "            lower and upper bounds for each variable seperately.)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
        "    \n",
        "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
        "        Minimize a function using modified Powell's method. This method\n",
        "        only uses function values, not derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each\n",
        "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        direc : ndarray, optional\n",
        "            Initial direction set.\n",
        "        xtol : float, optional\n",
        "            Line-search error tolerance.\n",
        "        ftol : float, optional\n",
        "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            If True, fopt, xi, direc, iter, funcalls, and\n",
        "            warnflag are returned.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence messages.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of the solution at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter which minimizes `func`.\n",
        "        fopt : number\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        direc : ndarray\n",
        "            Current direction set.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            Integer warning flag:\n",
        "                1 : Maximum number of function evaluations.\n",
        "                2 : Maximum number of iterations.\n",
        "        allvecs : list\n",
        "            List of solutions at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to unconstrained minimization algorithms for\n",
        "            multivariate functions. See the 'Powell' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a modification of Powell's method to find the minimum of\n",
        "        a function of N variables. Powell's method is a conjugate\n",
        "        direction method.\n",
        "        \n",
        "        The algorithm has two loops. The outer loop\n",
        "        merely iterates over the inner loop. The inner loop minimizes\n",
        "        over each current direction in the direction set. At the end\n",
        "        of the inner loop, if certain conditions are met, the direction\n",
        "        that gave the largest decrease is dropped and replaced with\n",
        "        the difference between the current estiamted x and the estimated\n",
        "        x from the beginning of the inner-loop.\n",
        "        \n",
        "        The technical conditions for replacing the direction of greatest\n",
        "        increase amount to checking that\n",
        "        \n",
        "        1. No further gain can be made along the direction of greatest increase\n",
        "           from that iteration.\n",
        "        2. The direction of greatest increase accounted for a large sufficient\n",
        "           fraction of the decrease in the function value from that iteration of\n",
        "           the inner loop.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
        "        function of several variables without calculating derivatives,\n",
        "        Computer Journal, 7 (2):155-162.\n",
        "        \n",
        "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
        "        Numerical Recipes (any edition), Cambridge University Press\n",
        "    \n",
        "    fmin_slsqp(func, x0, eqcons=[], f_eqcons=None, ieqcons=[], f_ieqcons=None, bounds=[], fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08)\n",
        "        Minimize a function using Sequential Least SQuares Programming\n",
        "        \n",
        "        Python interface function for the SLSQP Optimization subroutine\n",
        "        originally implemented by Dieter Kraft.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        x0 : 1-D ndarray of float\n",
        "            Initial guess for the independent variable(s).\n",
        "        eqcons : list\n",
        "            A list of functions of length n such that\n",
        "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_eqcons : callable f(x,*args)\n",
        "            Returns a 1-D array in which each element must equal 0.0 in a\n",
        "            successfully optimized problem.  If f_eqcons is specified,\n",
        "            eqcons is ignored.\n",
        "        ieqcons : list\n",
        "            A list of functions of length n such that\n",
        "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_ieqcons : callable f(x,*args)\n",
        "            Returns a 1-D ndarray in which each element must be greater or\n",
        "            equal to 0.0 in a successfully optimized problem.  If\n",
        "            f_ieqcons is specified, ieqcons is ignored.\n",
        "        bounds : list\n",
        "            A list of tuples specifying the lower and upper bound\n",
        "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
        "            Infinite values will be interpreted as large floating values.\n",
        "        fprime : callable `f(x,*args)`\n",
        "            A function that evaluates the partial derivatives of func.\n",
        "        fprime_eqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of equality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
        "        fprime_ieqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of inequality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
        "        args : sequence\n",
        "            Additional arguments passed to func and fprime.\n",
        "        iter : int\n",
        "            The maximum number of iterations.\n",
        "        acc : float\n",
        "            Requested accuracy.\n",
        "        iprint : int\n",
        "            The verbosity of fmin_slsqp :\n",
        "        \n",
        "            * iprint <= 0 : Silent operation\n",
        "            * iprint == 1 : Print summary upon completion (default)\n",
        "            * iprint >= 2 : Print status of each iterate and summary\n",
        "        disp : int\n",
        "            Over-rides the iprint interface (preferred).\n",
        "        full_output : bool\n",
        "            If False, return only the minimizer of func (default).\n",
        "            Otherwise, output final objective function and summary\n",
        "            information.\n",
        "        epsilon : float\n",
        "            The step size for finite-difference derivative estimates.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray of float\n",
        "            The final minimizer of func.\n",
        "        fx : ndarray of float, if full_output is true\n",
        "            The final value of the objective function.\n",
        "        its : int, if full_output is true\n",
        "            The number of iterations.\n",
        "        imode : int, if full_output is true\n",
        "            The exit mode from the optimizer (see below).\n",
        "        smode : string, if full_output is true\n",
        "            Message describing the exit mode from the optimizer.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'SLSQP' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Exit modes are defined as follows ::\n",
        "        \n",
        "            -1 : Gradient evaluation required (g & a)\n",
        "             0 : Optimization terminated successfully.\n",
        "             1 : Function evaluation required (f & c)\n",
        "             2 : More equality constraints than independent variables\n",
        "             3 : More than 3*n iterations in LSQ subproblem\n",
        "             4 : Inequality constraints incompatible\n",
        "             5 : Singular matrix E in LSQ subproblem\n",
        "             6 : Singular matrix C in LSQ subproblem\n",
        "             7 : Rank-deficient equality constraint subproblem HFTI\n",
        "             8 : Positive directional derivative for linesearch\n",
        "             9 : Iteration limit exceeded\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
        "    \n",
        "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
        "        Minimize a function with variables subject to bounds, using\n",
        "        gradient information in a truncated Newton algorithm. This\n",
        "        method wraps a C implementation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``func(x, *args)``\n",
        "            Function to minimize.  Must do one of:\n",
        "        \n",
        "            1. Return f and g, where f is the value of the function and g its\n",
        "               gradient (a list of floats).\n",
        "        \n",
        "            2. Return the function value but supply gradient function\n",
        "               seperately as `fprime`.\n",
        "        \n",
        "            3. Return the function value and set ``approx_grad=True``.\n",
        "        \n",
        "            If the function returns None, the minimization\n",
        "            is aborted.\n",
        "        x0 : array_like\n",
        "            Initial estimate of minimum.\n",
        "        fprime : callable ``fprime(x, *args)``\n",
        "            Gradient of `func`. If None, then either `func` must return the\n",
        "            function value and the gradient (``f,g = func(x, *args)``)\n",
        "            or `approx_grad` must be True.\n",
        "        args : tuple\n",
        "            Arguments to pass to function.\n",
        "        approx_grad : bool\n",
        "            If true, approximate the gradient numerically.\n",
        "        bounds : list\n",
        "            (min, max) pairs for each element in x0, defining the\n",
        "            bounds on that parameter. Use None or +/-inf for one of\n",
        "            min or max when there is no bound in that direction.\n",
        "        epsilon : float\n",
        "            Used if approx_grad is True. The stepsize in a finite\n",
        "            difference approximation for fprime.\n",
        "        scale : array_like\n",
        "            Scaling factors to apply to each variable.  If None, the\n",
        "            factors are up-low for interval bounded variables and\n",
        "            1+|x| for the others.  Defaults to None.\n",
        "        offset : array_like\n",
        "            Value to substract from each variable.  If None, the\n",
        "            offsets are (up+low)/2 for interval bounded variables\n",
        "            and x for the others.\n",
        "        messages :\n",
        "            Bit mask used to select messages display during\n",
        "            minimization values defined in the MSGS dict.  Defaults to\n",
        "            MGS_ALL.\n",
        "        disp : int\n",
        "            Integer interface to messages.  0 = no message, 5 = all messages\n",
        "        maxCGit : int\n",
        "            Maximum number of hessian*vector evaluations per main\n",
        "            iteration.  If maxCGit == 0, the direction chosen is\n",
        "            -gradient if maxCGit < 0, maxCGit is set to\n",
        "            max(1,min(50,n/2)).  Defaults to -1.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluation.  if None, maxfun is\n",
        "            set to max(100, 10*len(x0)).  Defaults to None.\n",
        "        eta : float\n",
        "            Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "            Defaults to -1.\n",
        "        stepmx : float\n",
        "            Maximum step for the line search.  May be increased during\n",
        "            call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "        accuracy : float\n",
        "            Relative precision for finite difference calculations.  If\n",
        "            <= machine_precision, set to sqrt(machine_precision).\n",
        "            Defaults to 0.\n",
        "        fmin : float\n",
        "            Minimum function value estimate.  Defaults to 0.\n",
        "        ftol : float\n",
        "            Precision goal for the value of f in the stoping criterion.\n",
        "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "        xtol : float\n",
        "            Precision goal for the value of x in the stopping\n",
        "            criterion (after applying x scaling factors).  If xtol <\n",
        "            0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "            -1.\n",
        "        pgtol : float\n",
        "            Precision goal for the value of the projected gradient in\n",
        "            the stopping criterion (after applying x scaling factors).\n",
        "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
        "            Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "        rescale : float\n",
        "            Scaling factor (in log10) used to trigger f value\n",
        "            rescaling.  If 0, rescale at each iteration.  If a large\n",
        "            value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution.\n",
        "        nfeval : int\n",
        "            The number of function evaluations.\n",
        "        rc : int\n",
        "            Return code as defined in the RCSTRINGS dict.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'TNC' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The underlying algorithm is truncated Newton, also called\n",
        "        Newton Conjugate-Gradient. This method differs from\n",
        "        scipy.optimize.fmin_ncg in that\n",
        "        \n",
        "        1. It wraps a C implementation of the algorithm\n",
        "        2. It allows each variable to be given an upper and lower bound.\n",
        "        \n",
        "        The algorithm incoporates the bound constraints by determining\n",
        "        the descent direction as in an unconstrained truncated Newton,\n",
        "        but never taking a step-size large enough to leave the space\n",
        "        of feasible x's. The algorithm keeps track of a set of\n",
        "        currently active constraints, and ignores them when computing\n",
        "        the minimum allowable step size. (The x's associated with the\n",
        "        active constraint are kept fixed.) If the maximum allowable\n",
        "        step size is zero then a new constraint is added. At the end\n",
        "        of each iteration one of the constraints may be deemed no\n",
        "        longer active and removed. A constraint is considered\n",
        "        no longer active is if it is currently active\n",
        "        but the gradient for that variable points inward from the\n",
        "        constraint. The specific constraint removed is the one\n",
        "        associated with the variable of largest index whose\n",
        "        constraint is no longer active.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
        "        \n",
        "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
        "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
        "    \n",
        "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
        "        Bounded minimization for scalar functions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized (must accept and return scalars).\n",
        "        x1, x2 : float or array scalar\n",
        "            The optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to function.\n",
        "        xtol : float, optional\n",
        "            The convergence tolerance.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations allowed.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        disp : int, optional\n",
        "            If non-zero, print messages.\n",
        "                0 : no message printing.\n",
        "                1 : non-convergence notification messages only.\n",
        "                2 : print a message on convergence too.\n",
        "                3 : print iteration results.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters (over given interval) which minimize the\n",
        "            objective function.\n",
        "        fval : number\n",
        "            The function value at the minimum point.\n",
        "        ierr : int\n",
        "            An error flag (0 if converged, 1 if maximum number of\n",
        "            function calls reached).\n",
        "        numfunc : int\n",
        "          The number of function calls made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Bounded' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Finds a local minimizer of the scalar function `func` in the\n",
        "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
        "        for auto-bracketing).\n",
        "    \n",
        "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
        "        Find the roots of a function.\n",
        "        \n",
        "        Return the roots of the (non-linear) equations defined by\n",
        "        ``func(x) = 0`` given a starting estimate.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            A function that takes at least one (possibly vector) argument.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the roots of ``func(x) = 0``.\n",
        "        args : tuple, optional\n",
        "            Any extra arguments to `func`.\n",
        "        fprime : callable(x), optional\n",
        "            A function to compute the Jacobian of `func` with derivatives\n",
        "            across the rows. By default, the Jacobian will be estimated.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        col_deriv : bool, optional\n",
        "            Specify whether the Jacobian function computes derivatives down\n",
        "            the columns (faster, because there is no transpose operation).\n",
        "        xtol : float\n",
        "            The calculation will terminate if the relative error between two\n",
        "            consecutive iterates is at most `xtol`.\n",
        "        maxfev : int, optional\n",
        "            The maximum number of calls to the function. If zero, then\n",
        "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "            in `x0`.\n",
        "        band : tuple, optional\n",
        "            If set to a two-sequence containing the number of sub- and\n",
        "            super-diagonals within the band of the Jacobi matrix, the\n",
        "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "        epsfcn : float, optional\n",
        "            A suitable step length for the forward-difference\n",
        "            approximation of the Jacobian (for ``fprime=None``). If\n",
        "            `epsfcn` is less than the machine precision, it is assumed\n",
        "            that the relative errors in the functions are of the order of\n",
        "            the machine precision.\n",
        "        factor : float, optional\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``).  Should be in the interval\n",
        "            ``(0.1, 100)``.\n",
        "        diag : sequence, optional\n",
        "            N positive entries that serve as a scale factors for the\n",
        "            variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for\n",
        "            an unsuccessful call).\n",
        "        infodict : dict\n",
        "            A dictionary of optional outputs with the keys:\n",
        "        \n",
        "            ``nfev``\n",
        "                number of function calls\n",
        "            ``njev``\n",
        "                number of Jacobian calls\n",
        "            ``fvec``\n",
        "                function evaluated at the output\n",
        "            ``fjac``\n",
        "                the orthogonal matrix, q, produced by the QR\n",
        "                factorization of the final approximate Jacobian\n",
        "                matrix, stored column wise\n",
        "            ``r``\n",
        "                upper triangular matrix produced by QR factorization\n",
        "                of the same matrix\n",
        "            ``qtf``\n",
        "                the vector ``(transpose(q) * fvec)``\n",
        "        \n",
        "        ier : int\n",
        "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
        "            to `mesg` for more information.\n",
        "        mesg : str\n",
        "            If no solution is found, `mesg` details the cause of failure.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        root : Interface to root finding algorithms for multivariate\n",
        "        functions. See the 'hybr' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
        "    \n",
        "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0)\n",
        "        Return the minimum of a function of one variable.\n",
        "        \n",
        "        Given a function of one variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            Objective function to minimize.\n",
        "        args : tuple\n",
        "            Additional arguments (if present), passed to func.\n",
        "        brack : tuple\n",
        "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,\n",
        "            c), then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            x tolerance stop criterion\n",
        "        full_output : bool\n",
        "            If True, return optional outputs.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Golden' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses analog of bisection method to decrease the bracketed\n",
        "        interval.\n",
        "    \n",
        "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
        "        Minimize the sum of squares of a set of equations.\n",
        "        \n",
        "        ::\n",
        "        \n",
        "            x = arg min(sum(func(y)**2,axis=0))\n",
        "                     y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            should take at least one (possibly length N vector) argument and\n",
        "            returns M floating point numbers.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the minimization.\n",
        "        args : tuple\n",
        "            Any extra arguments to func are placed in this tuple.\n",
        "        Dfun : callable\n",
        "            A function or method to compute the Jacobian of func with derivatives\n",
        "            across the rows. If this is None, the Jacobian will be estimated.\n",
        "        full_output : bool\n",
        "            non-zero to return all optional outputs.\n",
        "        col_deriv : bool\n",
        "            non-zero to specify that the Jacobian function computes derivatives\n",
        "            down the columns (faster, because there is no transpose operation).\n",
        "        ftol : float\n",
        "            Relative error desired in the sum of squares.\n",
        "        xtol : float\n",
        "            Relative error desired in the approximate solution.\n",
        "        gtol : float\n",
        "            Orthogonality desired between the function vector and the columns of\n",
        "            the Jacobian.\n",
        "        maxfev : int\n",
        "            The maximum number of calls to the function. If zero, then 100*(N+1) is\n",
        "            the maximum where N is the number of elements in x0.\n",
        "        epsfcn : float\n",
        "            A suitable step length for the forward-difference approximation of the\n",
        "            Jacobian (for Dfun=None). If epsfcn is less than the machine precision,\n",
        "            it is assumed that the relative errors in the functions are of the\n",
        "            order of the machine precision.\n",
        "        factor : float\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "        diag : sequence\n",
        "            N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for an unsuccessful\n",
        "            call).\n",
        "        cov_x : ndarray\n",
        "            Uses the fjac and ipvt optional outputs to construct an\n",
        "            estimate of the jacobian around the solution. None if a\n",
        "            singular matrix encountered (indicates very flat curvature in\n",
        "            some direction).  This matrix must be multiplied by the\n",
        "            residual variance to get the covariance of the\n",
        "            parameter estimates -- see curve_fit.\n",
        "        infodict : dict\n",
        "            a dictionary of optional outputs with the key s:\n",
        "        \n",
        "            ``nfev``\n",
        "                The number of function calls\n",
        "            ``fvec``\n",
        "                The function evaluated at the output\n",
        "            ``fjac``\n",
        "                A permutation of the R matrix of a QR\n",
        "                factorization of the final approximate\n",
        "                Jacobian matrix, stored column wise.\n",
        "                Together with ipvt, the covariance of the\n",
        "                estimate can be approximated.\n",
        "            ``ipvt``\n",
        "                An integer array of length N which defines\n",
        "                a permutation matrix, p, such that\n",
        "                fjac*p = q*r, where r is upper triangular\n",
        "                with diagonal elements of nonincreasing\n",
        "                magnitude. Column j of p is column ipvt(j)\n",
        "                of the identity matrix.\n",
        "            ``qtf``\n",
        "                The vector (transpose(q) * fvec).\n",
        "        \n",
        "        mesg : str\n",
        "            A string message giving information about the cause of failure.\n",
        "        ier : int\n",
        "            An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\n",
        "            found.  Otherwise, the solution was not found. In either case, the\n",
        "            optional output variable 'mesg' gives more information.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
        "        \n",
        "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
        "        objective function.\n",
        "        This approximation assumes that the objective function is based on the\n",
        "        difference between some observed target data (ydata) and a (non-linear)\n",
        "        function of the parameters `f(xdata, params)` ::\n",
        "        \n",
        "               func(params) = ydata - f(xdata, params)\n",
        "        \n",
        "        so that the objective function is ::\n",
        "        \n",
        "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
        "             params\n",
        "    \n",
        "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)\n",
        "        Find alpha that satisfies strong Wolfe conditions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function.\n",
        "        myfprime : callable f'(x,*args)\n",
        "            Objective function gradient.\n",
        "        xk : ndarray\n",
        "            Starting point.\n",
        "        pk : ndarray\n",
        "            Search direction.\n",
        "        gfk : ndarray, optional\n",
        "            Gradient value for x=xk (xk being the current parameter\n",
        "            estimate). Will be recomputed if omitted.\n",
        "        old_fval : float, optional\n",
        "            Function value for x=xk. Will be recomputed if omitted.\n",
        "        old_old_fval : float, optional\n",
        "            Function value for the point preceding x=xk\n",
        "        args : tuple, optional\n",
        "            Additional arguments passed to objective function.\n",
        "        c1 : float, optional\n",
        "            Parameter for Armijo condition rule.\n",
        "        c2 : float, optional\n",
        "            Parameter for curvature condition rule.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        alpha0 : float\n",
        "            Alpha for which ``x_new = x0 + alpha * pk``.\n",
        "        fc : int\n",
        "            Number of function evaluations made.\n",
        "        gc : int\n",
        "            Number of gradient evaluations made.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses the line search algorithm to enforce strong Wolfe\n",
        "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
        "        1999, pg. 59-60.\n",
        "        \n",
        "        For the zoom phase it uses an algorithm by [...].\n",
        "    \n",
        "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a scalar Jacobian approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            The Jacobian approximation is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    minimize(fun, x0, args=(), method='BFGS', jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
        "        Minimization of scalar function of one or more variables.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its\n",
        "            derivatives (Jacobian, Hessian).\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Nelder-Mead'\n",
        "                - 'Powell'\n",
        "                - 'CG'\n",
        "                - 'BFGS'\n",
        "                - 'Newton-CG'\n",
        "                - 'Anneal'\n",
        "                - 'L-BFGS-B'\n",
        "                - 'TNC'\n",
        "                - 'COBYLA'\n",
        "                - 'SLSQP'\n",
        "                - 'dogleg'\n",
        "                - 'trust-ncg'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            Jacobian of objective function. Only for CG, BFGS, Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of the\n",
        "            objective. In this case, it must accept the same arguments as `fun`.\n",
        "        hess, hessp : callable, optional\n",
        "            Hessian of objective function or Hessian of objective function\n",
        "            times an arbitrary vector p.  Only for Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
        "            provided, then `hessp` will be ignored.  If neither `hess` nor\n",
        "            `hessp` is provided, then the hessian product will be approximated\n",
        "            using finite differences on `jac`. `hessp` must compute the Hessian\n",
        "            times an arbitrary vector.\n",
        "        bounds : sequence, optional\n",
        "            Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        constraints : dict or sequence of dict, optional\n",
        "            Constraints definition (only for COBYLA and SLSQP).\n",
        "            Each constraint is defined in a dictionary with fields:\n",
        "                type : str\n",
        "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
        "                fun : callable\n",
        "                    The function defining the constraint.\n",
        "                jac : callable, optional\n",
        "                    The Jacobian of `fun` (only for SLSQP).\n",
        "                args : sequence, optional\n",
        "                    Extra arguments to be passed to the function and Jacobian.\n",
        "            Equality constraint means that the constraint function result is to\n",
        "            be zero whereas inequality means that it is to be non-negative.\n",
        "            Note that COBYLA only supports inequality constraints.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. All methods accept the following\n",
        "            generic options:\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "            For method-specific options, see `show_options('minimize', method)`.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *BFGS*.\n",
        "        \n",
        "        **Unconstrained minimization**\n",
        "        \n",
        "        Method *Nelder-Mead* uses the Simplex algorithm [1]_, [2]_. This\n",
        "        algorithm has been successful in many applications but other algorithms\n",
        "        using the first and/or second derivatives information might be preferred\n",
        "        for their better performances and robustness in general.\n",
        "        \n",
        "        Method *Powell* is a modification of Powell's method [3]_, [4]_ which\n",
        "        is a conjugate direction method. It performs sequential one-dimensional\n",
        "        minimizations along each vector of the directions set (`direc` field in\n",
        "        `options` and `info`), which is updated at each iteration of the main\n",
        "        minimization loop. The function need not be differentiable, and no\n",
        "        derivatives are taken.\n",
        "        \n",
        "        Method *CG* uses a nonlinear conjugate gradient algorithm by Polak and\n",
        "        Ribiere, a variant of the Fletcher-Reeves method described in [5]_ pp.\n",
        "        120-122. Only the first derivatives are used.\n",
        "        \n",
        "        Method *BFGS* uses the quasi-Newton method of Broyden, Fletcher,\n",
        "        Goldfarb, and Shanno (BFGS) [5]_ pp. 136. It uses the first derivatives\n",
        "        only. BFGS has proven good performance even for non-smooth\n",
        "        optimizations. This method also returns an approximation of the Hessian\n",
        "        inverse, stored as `hess_inv` in the Result object.\n",
        "        \n",
        "        Method *Newton-CG* uses a Newton-CG algorithm [5]_ pp. 168 (also known\n",
        "        as the truncated Newton method). It uses a CG method to the compute the\n",
        "        search direction. See also *TNC* method for a box-constrained\n",
        "        minimization with a similar algorithm.\n",
        "        \n",
        "        Method *Anneal* uses simulated annealing, which is a probabilistic\n",
        "        metaheuristic algorithm for global optimization. It uses no derivative\n",
        "        information from the function being optimized.\n",
        "        \n",
        "        Method *dogleg* uses the dog-leg trust-region algorithm [5]_\n",
        "        for unconstrained minimization. This algorithm requires the gradient\n",
        "        and Hessian; furthermore the Hessian is required to be positive definite.\n",
        "        \n",
        "        Method *trust-ncg* uses the Newton conjugate gradient trust-region\n",
        "        algorithm [5]_ for unconstrained minimization. This algorithm requires\n",
        "        the gradient and either the Hessian or a function that computes the\n",
        "        product of the Hessian with a given vector.\n",
        "        \n",
        "        **Constrained minimization**\n",
        "        \n",
        "        Method *L-BFGS-B* uses the L-BFGS-B algorithm [6]_, [7]_ for bound\n",
        "        constrained minimization.\n",
        "        \n",
        "        Method *TNC* uses a truncated Newton algorithm [5]_, [8]_ to minimize a\n",
        "        function with variables subject to bounds. This algorithm uses\n",
        "        gradient information; it is also called Newton Conjugate-Gradient. It\n",
        "        differs from the *Newton-CG* method described above as it wraps a C\n",
        "        implementation and allows each variable to be given upper and lower\n",
        "        bounds.\n",
        "        \n",
        "        Method *COBYLA* uses the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method [9]_, [10]_, [11]_. The algorithm is\n",
        "        based on linear approximations to the objective function and each\n",
        "        constraint. The method wraps a FORTRAN implementation of the algorithm.\n",
        "        \n",
        "        Method *SLSQP* uses Sequential Least SQuares Programming to minimize a\n",
        "        function of several variables with any combination of bounds, equality\n",
        "        and inequality constraints. The method wraps the SLSQP Optimization\n",
        "        subroutine originally implemented by Dieter Kraft [12]_. Note that the\n",
        "        wrapper handles infinite values in bounds by converting them into large\n",
        "        floating values.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
        "            Minimization. The Computer Journal 7: 308-13.\n",
        "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
        "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
        "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
        "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
        "            191-208.\n",
        "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
        "           a function of several variables without calculating derivatives. The\n",
        "           Computer Journal 7: 155-162.\n",
        "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
        "           Numerical Recipes (any edition), Cambridge University Press.\n",
        "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
        "           Springer New York.\n",
        "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
        "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
        "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
        "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
        "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
        "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
        "           550-560.\n",
        "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
        "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
        "        .. [9] Powell, M J D. A direct search optimization method that models\n",
        "           the objective and constraint functions by linear interpolation.\n",
        "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
        "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
        "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
        "           calculations. 1998. Acta Numerica 7: 287-336.\n",
        "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
        "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
        "           2007/NA03\n",
        "        .. [12] Kraft, D. A software package for sequential quadratic\n",
        "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
        "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
        "        function (and its respective derivatives) is implemented in `rosen`\n",
        "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
        "        \n",
        "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
        "        \n",
        "        A simple application of the *Nelder-Mead* method is:\n",
        "        \n",
        "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
        "        >>> res = minimize(rosen, x0, method='Nelder-Mead')\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        \n",
        "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
        "        options:\n",
        "        \n",
        "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
        "        ...                options={'gtol': 1e-6, 'disp': True})\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 0.000000\n",
        "                 Iterations: 52\n",
        "                 Function evaluations: 64\n",
        "                 Gradient evaluations: 64\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        >>> print res.message\n",
        "        Optimization terminated successfully.\n",
        "        >>> res.hess\n",
        "        [[ 0.00749589  0.01255155  0.02396251  0.04750988  0.09495377]\n",
        "         [ 0.01255155  0.02510441  0.04794055  0.09502834  0.18996269]\n",
        "         [ 0.02396251  0.04794055  0.09631614  0.19092151  0.38165151]\n",
        "         [ 0.04750988  0.09502834  0.19092151  0.38341252  0.7664427 ]\n",
        "         [ 0.09495377  0.18996269  0.38165151  0.7664427   1.53713523]]\n",
        "        \n",
        "        \n",
        "        Next, consider a minimization problem with several constraints (namely\n",
        "        Example 16.4 from [5]_). The objective function is:\n",
        "        \n",
        "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
        "        \n",
        "        There are three constraints defined as:\n",
        "        \n",
        "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
        "        \n",
        "        And variables must be positive, hence the following bounds:\n",
        "        \n",
        "        >>> bnds = ((0, None), (0, None))\n",
        "        \n",
        "        The optimization problem is solved using the SLSQP method as:\n",
        "        \n",
        "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
        "        ...                constraints=cons)\n",
        "        \n",
        "        It should converge to the theoretical solution (1.4 ,1.7).\n",
        "    \n",
        "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
        "        Minimization of scalar function of one variable.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "            Scalar function, must return a scalar.\n",
        "        bracket : sequence, optional\n",
        "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
        "            interval and can either have three items `(a, b, c)` so that `a < b\n",
        "            < c` and `fun(b) < fun(a), fun(c)` or two items `a` and `c` which\n",
        "            are assumed to be a starting interval for a downhill bracket search\n",
        "            (see `bracket`); it doesn't always mean that the obtained solution\n",
        "            will satisfy `a <= x <= c`.\n",
        "        bounds : sequence, optional\n",
        "            For method 'bounded', `bounds` is mandatory and must have two items\n",
        "            corresponding to the optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Brent'\n",
        "                - 'Bounded'\n",
        "                - 'Golden'\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options.\n",
        "                xtol : float\n",
        "                    Relative error in solution `xopt` acceptable for\n",
        "                    convergence.\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for scalar multivariate\n",
        "            functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *Brent*.\n",
        "        \n",
        "        Method *Brent* uses Brent's algorithm to find a local minimum.\n",
        "        The algorithm uses inverse parabolic interpolation when possible to\n",
        "        speed up convergence of the golden section method.\n",
        "        \n",
        "        Method *Golden* uses the golden section search technique. It uses\n",
        "        analog of the bisection method to decrease the bracketed interval. It\n",
        "        is usually preferable to use the *Brent* method.\n",
        "        \n",
        "        Method *Bounded* can perform bounded minimization. It uses the Brent\n",
        "        method to find a local minimum in the interval x1 < xopt < x2.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Consider the problem of minimizing the following function.\n",
        "        \n",
        "        >>> def f(x):\n",
        "        ...     return (x - 2) * x * (x + 2)**2\n",
        "        \n",
        "        Using the *Brent* method, we find the local minimum as:\n",
        "        \n",
        "        >>> from scipy.optimize import minimize_scalar\n",
        "        >>> res = minimize_scalar(f)\n",
        "        >>> res.x\n",
        "        1.28077640403\n",
        "        \n",
        "        Using the *Bounded* method, we find a local minimum with specified\n",
        "        bounds as:\n",
        "        \n",
        "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
        "        >>> res.x\n",
        "        -2.0000002026\n",
        "    \n",
        "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)\n",
        "        Find a zero using the Newton-Raphson or secant method.\n",
        "        \n",
        "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
        "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
        "        is provided, otherwise the secant method is used.  If the second order\n",
        "        derivate `fprime2` of `func` is provided, parabolic Halley's method\n",
        "        is used.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            The function whose zero is wanted. It must be a function of a\n",
        "            single variable of the form f(x,a,b,c...), where a,b,c... are extra\n",
        "            arguments that can be passed in the `args` parameter.\n",
        "        x0 : float\n",
        "            An initial estimate of the zero that should be somewhere near the\n",
        "            actual zero.\n",
        "        fprime : function, optional\n",
        "            The derivative of the function when available and convenient. If it\n",
        "            is None (default), then the secant method is used.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to be used in the function call.\n",
        "        tol : float, optional\n",
        "            The allowable error of the zero value.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations.\n",
        "        fprime2 : function, optional\n",
        "            The second order derivative of the function when available and\n",
        "            convenient. If it is None (default), then the normal Newton-Raphson\n",
        "            or the secant method is used. If it is given, parabolic Halley's\n",
        "            method is used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        zero : float\n",
        "            Estimated location where function is zero.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, ridder, bisect\n",
        "        fsolve : find zeroes in n dimensions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The convergence rate of the Newton-Raphson method is quadratic,\n",
        "        the Halley method is cubic, and the secant method is\n",
        "        sub-quadratic.  This means that if the function is well behaved\n",
        "        the actual error in the estimated zero is approximately the square\n",
        "        (cube for Halley) of the requested tolerance up to roundoff\n",
        "        error. However, the stopping criterion used here is the step size\n",
        "        and there is no guarantee that a zero has been found. Consequently\n",
        "        the result should be verified. Safer algorithms are brentq,\n",
        "        brenth, ridder, and bisect, but they all require that the root\n",
        "        first be bracketed in an interval where the function changes\n",
        "        sign. The brentq algorithm is recommended for general use in one\n",
        "        dimensional problems when such an interval has been found.\n",
        "    \n",
        "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
        "        \n",
        "        This method is suitable for solving large-scale problems.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        rdiff : float, optional\n",
        "            Relative step size to use in numerical differentiation.\n",
        "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
        "            Krylov method to use to approximate the Jacobian.\n",
        "            Can be a string, or a function implementing the same interface as\n",
        "            the iterative solvers in `scipy.sparse.linalg`.\n",
        "        \n",
        "            The default is `scipy.sparse.linalg.lgmres`.\n",
        "        inner_M : LinearOperator or InverseJacobian\n",
        "            Preconditioner for the inner Krylov iteration.\n",
        "            Note that you can use also inverse Jacobians as (adaptive)\n",
        "            preconditioners. For example,\n",
        "        \n",
        "            >>> jac = BroydenFirst()\n",
        "            >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "            If the preconditioner has a method named 'update', it will be called\n",
        "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
        "            the current point, and ``f`` the current function value.\n",
        "        inner_tol, inner_maxiter, ...\n",
        "            Parameters to pass on to the \\\"inner\\\" Krylov solver.\n",
        "            See `scipy.sparse.linalg.gmres` for details.\n",
        "        outer_k : int, optional\n",
        "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
        "            See `scipy.sparse.linalg.lgmres` for details.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.sparse.linalg.gmres\n",
        "        scipy.sparse.linalg.lgmres\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This function implements a Newton-Krylov solver. The basic idea is\n",
        "        to compute the inverse of the Jacobian with an iterative Krylov\n",
        "        method. These methods require only evaluating the Jacobian-vector\n",
        "        products, which are conveniently approximated by numerical\n",
        "        differentiation:\n",
        "        \n",
        "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
        "        \n",
        "        Due to the use of iterative matrix inverses, these methods can\n",
        "        deal with large nonlinear problems.\n",
        "        \n",
        "        Scipy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
        "        solvers to choose from. The default here is `lgmres`, which is a\n",
        "        variant of restarted GMRES iteration that reuses some of the\n",
        "        information obtained in the previous Newton steps to invert\n",
        "        Jacobians in subsequent steps.\n",
        "        \n",
        "        For a review on Newton-Krylov methods, see for example [KK]_,\n",
        "        and for the LGMRES sparse inverse method, see [BJM]_.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [KK] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2003).\n",
        "        .. [BJM] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
        "                 SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
        "    \n",
        "    nnls(A, b)\n",
        "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
        "        for a FORTAN non-negative least squares solver.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : ndarray\n",
        "            Matrix ``A`` as shown above.\n",
        "        b : ndarray\n",
        "            Right-hand side vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            Solution vector.\n",
        "        rnorm : float\n",
        "            The residual, ``|| Ax-b ||_2``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The FORTRAN code was published in the book below. The algorithm\n",
        "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
        "        conditions for the non-negative least squares problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
        "    \n",
        "    ridder(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in an interval.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.\n",
        "            In particular, ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton : one-dimensional root-finding\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
        "        generally as fast as the Brent rountines. [Ridders1979]_ provides the\n",
        "        classic description and source of the algorithm. A description can also be\n",
        "        found in any recent edition of Numerical Recipes.\n",
        "        \n",
        "        The routine used here diverges slightly from standard presentations in\n",
        "        order to be a bit more careful of tolerance.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ridders1979]\n",
        "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
        "           Single Root of a Real Continuous Function.\"\n",
        "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
        "    \n",
        "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
        "        Find a root of a vector function.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            A vector function to find a root of.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its Jacobian.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'hybr'\n",
        "                - 'lm'\n",
        "                - 'broyden1'\n",
        "                - 'broyden2'\n",
        "                - 'anderson'\n",
        "                - 'linearmixing'\n",
        "                - 'diagbroyden'\n",
        "                - 'excitingmixing'\n",
        "                - 'krylov'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
        "            this case, it must accept the same arguments as `fun`.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. E.g. `xtol` or `maxiter`, see\n",
        "            ``show_options('root', method)`` for details.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : Result\n",
        "            The solution represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the algorithm exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *hybr*.\n",
        "        \n",
        "        Method *hybr* uses a modification of the Powell hybrid method as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Method *lm* solves the system of nonlinear equations in a least squares\n",
        "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
        "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
        "        with backtracking or full line searches [2]_. Each method corresponds\n",
        "        to a particular Jacobian approximations. See `nonlin` for details.\n",
        "        \n",
        "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
        "          known as Broyden's good method.\n",
        "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
        "          is known as Broyden's bad method.\n",
        "        - Method *anderson* uses (extended) Anderson mixing.\n",
        "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
        "          is suitable for large-scale problem.\n",
        "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
        "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
        "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
        "          approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "            The algorithms implemented for methods *diagbroyden*,\n",
        "            *linearmixing* and *excitingmixing* may be useful for specific\n",
        "            problems, but whether they will work may depend strongly on the\n",
        "            problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
        "           1980. User Guide for MINPACK-1.\n",
        "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
        "            Equations. Society for Industrial and Applied Mathematics.\n",
        "            <http://www.siam.org/books/kelley/>\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following functions define a system of nonlinear equations and its\n",
        "        jacobian.\n",
        "        \n",
        "        >>> def fun(x):\n",
        "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
        "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
        "        \n",
        "        >>> def jac(x):\n",
        "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
        "        ...                       -1.5 * (x[0] - x[1])**2],\n",
        "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
        "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
        "        \n",
        "        A solution can be obtained as follows.\n",
        "        \n",
        "        >>> from scipy import optimize\n",
        "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
        "        >>> sol.x\n",
        "        array([ 0.8411639,  0.1588361])\n",
        "    \n",
        "    rosen(x)\n",
        "        The Rosenbrock function.\n",
        "        \n",
        "        The function computed is::\n",
        "        \n",
        "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Rosenbrock function is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        f : float\n",
        "            The value of the Rosenbrock function.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen_der, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_der(x)\n",
        "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the derivative is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_der : (N,) ndarray\n",
        "            The gradient of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess(x)\n",
        "        The Hessian matrix of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess_prod(x, p)\n",
        "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        p : array_like\n",
        "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess_prod : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
        "            by the vector `p`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess\n",
        "    \n",
        "    show_options(solver, method=None)\n",
        "        Show documentation for additional options of optimization solvers.\n",
        "        \n",
        "        These are method-specific options that can be supplied through the\n",
        "        ``options`` dict.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        solver : str\n",
        "            Type of optimization solver. One of {`minimize`, `root`}.\n",
        "        method : str, optional\n",
        "            If not given, shows all methods of the specified solver. Otherwise,\n",
        "            show only the options for the specified method. Valid values\n",
        "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
        "            'minimize').\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \n",
        "        ** minimize options\n",
        "        \n",
        "        * BFGS options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Nelder-Mead options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "        \n",
        "        * Newton-CG options:\n",
        "            xtol : float\n",
        "                Average relative error in solution `xopt` acceptable for\n",
        "                convergence.\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * CG options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Powell options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            direc : ndarray\n",
        "                Initial set of direction vectors for the Powell method.\n",
        "        \n",
        "        * Anneal options:\n",
        "            ftol : float\n",
        "                Relative error in ``fun(x)`` acceptable for convergence.\n",
        "            schedule : str\n",
        "                Annealing schedule to use. One of: 'fast', 'cauchy' or\n",
        "                'boltzmann'.\n",
        "            T0 : float\n",
        "                Initial Temperature (estimated as 1.2 times the largest\n",
        "                cost-function deviation over random points in the range).\n",
        "            Tf : float\n",
        "                Final goal temperature.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            maxaccept : int\n",
        "                Maximum changes to accept.\n",
        "            boltzmann : float\n",
        "                Boltzmann constant in acceptance test (increase for less\n",
        "                stringent test at each temperature).\n",
        "            learn_rate : float\n",
        "                Scale constant for adjusting guesses.\n",
        "            quench, m, n : float\n",
        "                Parameters to alter fast_sa schedule.\n",
        "            lower, upper : float or ndarray\n",
        "                Lower and upper bounds on `x`.\n",
        "            dwell : int\n",
        "                The number of times to search the space at each temperature.\n",
        "        \n",
        "        * L-BFGS-B options:\n",
        "            ftol : float\n",
        "                The iteration stops when ``(f^k -\n",
        "                f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
        "            gtol : float\n",
        "                The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
        "                <= gtol`` where ``pg_i`` is the i-th component of the\n",
        "                projected gradient.\n",
        "            maxcor : int\n",
        "                The maximum number of variable metric corrections used to\n",
        "                define the limited memory matrix. (The limited memory BFGS\n",
        "                method does not store the full hessian but uses this many terms\n",
        "                in an approximation to it.)\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * TNC options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stoping criterion.\n",
        "                If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "            xtol : float\n",
        "                Precision goal for the value of x in the stopping\n",
        "                criterion (after applying x scaling factors).  If xtol <\n",
        "                0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "                -1.\n",
        "            gtol : float\n",
        "                Precision goal for the value of the projected gradient in\n",
        "                the stopping criterion (after applying x scaling factors).\n",
        "                If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
        "                Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "            scale : list of floats\n",
        "                Scaling factors to apply to each variable.  If None, the\n",
        "                factors are up-low for interval bounded variables and\n",
        "                1+|x] fo the others.  Defaults to None\n",
        "            offset : float\n",
        "                Value to substract from each variable.  If None, the\n",
        "                offsets are (up+low)/2 for interval bounded variables\n",
        "                and x for the others.\n",
        "            maxCGit : int\n",
        "                Maximum number of hessian*vector evaluations per main\n",
        "                iteration.  If maxCGit == 0, the direction chosen is\n",
        "                -gradient if maxCGit < 0, maxCGit is set to\n",
        "                max(1,min(50,n/2)).  Defaults to -1.\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluation.  if None, `maxiter` is\n",
        "                set to max(100, 10*len(x0)).  Defaults to None.\n",
        "            eta : float\n",
        "                Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "                Defaults to -1.\n",
        "            stepmx : float\n",
        "                Maximum step for the line search.  May be increased during\n",
        "                call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "            accuracy : float\n",
        "                Relative precision for finite difference calculations.  If\n",
        "                <= machine_precision, set to sqrt(machine_precision).\n",
        "                Defaults to 0.\n",
        "            minfev : float\n",
        "                Minimum function value estimate.  Defaults to 0.\n",
        "            rescale : float\n",
        "                Scaling factor (in log10) used to trigger f value\n",
        "                rescaling.  If 0, rescale at each iteration.  If a large\n",
        "                value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        \n",
        "        * COBYLA options:\n",
        "            tol : float\n",
        "                Final accuracy in the optimization (not precisely guaranteed).\n",
        "                This is a lower bound on the size of the trust region.\n",
        "            rhobeg : float\n",
        "                Reasonable initial changes to the variables.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * SLSQP options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stopping criterion.\n",
        "            eps : float\n",
        "                Step size used for numerical approximation of the jacobian.\n",
        "            maxiter : int\n",
        "                Maximum number of iterations.\n",
        "        \n",
        "        * dogleg options:\n",
        "            initial_trust_radius : float\n",
        "                Initial trust-region radius.\n",
        "            max_trust_radius : float\n",
        "                Maximum value of the trust-region radius. No steps that are longer\n",
        "                than this value will be proposed.\n",
        "            eta : float\n",
        "                Trust region related acceptance stringency for proposed steps.\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "        \n",
        "        * trust-ncg options:\n",
        "            see dogleg options.\n",
        "        \n",
        "        ** root options\n",
        "        \n",
        "        * hybrd options:\n",
        "            col_deriv : bool\n",
        "                Specify whether the Jacobian function computes derivatives down\n",
        "                the columns (faster, because there is no transpose operation).\n",
        "            xtol : float\n",
        "                The calculation will terminate if the relative error between\n",
        "                two consecutive iterates is at most `xtol`.\n",
        "            maxfev : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "                in `x0`.\n",
        "            band : sequence\n",
        "                If set to a two-sequence containing the number of sub- and\n",
        "                super-diagonals within the band of the Jacobi matrix, the\n",
        "                Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation\n",
        "                of the Jacobian (for ``fprime=None``). If `epsfcn` is less than\n",
        "                the machine precision, it is assumed that the relative errors\n",
        "                in the functions are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound (``factor * ||\n",
        "                diag * x||``).  Should be in the interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the\n",
        "                variables.\n",
        "        \n",
        "        * LM options:\n",
        "            col_deriv : bool\n",
        "                non-zero to specify that the Jacobian function computes derivatives\n",
        "                down the columns (faster, because there is no transpose operation).\n",
        "            ftol : float\n",
        "                Relative error desired in the sum of squares.\n",
        "            xtol : float\n",
        "                Relative error desired in the approximate solution.\n",
        "            gtol : float\n",
        "                Orthogonality desired between the function vector and the columns\n",
        "                of the Jacobian.\n",
        "            maxiter : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                100*(N+1) is the maximum where N is the number of elements in x0.\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation of\n",
        "                the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
        "                precision, it is assumed that the relative errors in the functions\n",
        "                are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound\n",
        "                (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        * Broyden1 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Broyden2 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Anderson options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    M : float, optional\n",
        "                        Number of previous vectors to retain. Defaults to 5.\n",
        "                    w0 : float, optional\n",
        "                        Regularization parameter for numerical stability.\n",
        "                        Compared to unity, good values of the order of 0.01.\n",
        "        \n",
        "        * LinearMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * DiagBroyden options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * ExcitingMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial Jacobian approximation is (-1/alpha).\n",
        "                    alphamax : float, optional\n",
        "                        The entries of the diagonal Jacobian are kept in the range\n",
        "                        ``[alpha, alphamax]``.\n",
        "        \n",
        "        * Krylov options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    rdiff : float, optional\n",
        "                        Relative step size to use in numerical differentiation.\n",
        "                    method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or\n",
        "                        function\n",
        "                        Krylov method to use to approximate the Jacobian.\n",
        "                        Can be a string, or a function implementing the same\n",
        "                        interface as the iterative solvers in\n",
        "                        `scipy.sparse.linalg`.\n",
        "        \n",
        "                        The default is `scipy.sparse.linalg.lgmres`.\n",
        "                    inner_M : LinearOperator or InverseJacobian\n",
        "                        Preconditioner for the inner Krylov iteration.\n",
        "                        Note that you can use also inverse Jacobians as (adaptive)\n",
        "                        preconditioners. For example,\n",
        "        \n",
        "                        >>> jac = BroydenFirst()\n",
        "                        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "                        If the preconditioner has a method named 'update', it will\n",
        "                        be called as ``update(x, f)`` after each nonlinear step,\n",
        "                        with ``x`` giving the current point, and ``f`` the current\n",
        "                        function value.\n",
        "                    inner_tol, inner_maxiter, ...\n",
        "                        Parameters to pass on to the \"inner\" Krylov solver.\n",
        "                        See `scipy.sparse.linalg.gmres` for details.\n",
        "                    outer_k : int, optional\n",
        "                        Size of the subspace kept across LGMRES nonlinear\n",
        "                        iterations.\n",
        "        \n",
        "                        See `scipy.sparse.linalg.lgmres` for details.\n",
        "\n",
        "DATA\n",
        "    __all__ = ['OptimizeWarning', 'Result', 'absolute_import', 'anderson',...\n",
        "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}